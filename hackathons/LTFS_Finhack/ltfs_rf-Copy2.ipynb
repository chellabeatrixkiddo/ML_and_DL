{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\", parse_dates=['DisbursalDate', 'Date.of.Birth'])\n",
    "test = pd.read_csv(\"test.csv\", parse_dates=['DisbursalDate', 'Date.of.Birth'])\n",
    "\n",
    "submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniqueID                                  0\n",
       "disbursed_amount                          0\n",
       "asset_cost                                0\n",
       "ltv                                       0\n",
       "branch_id                                 0\n",
       "supplier_id                               0\n",
       "manufacturer_id                           0\n",
       "Current_pincode_ID                        0\n",
       "Date.of.Birth                             0\n",
       "Employment.Type                        7661\n",
       "DisbursalDate                             0\n",
       "State_ID                                  0\n",
       "Employee_code_ID                          0\n",
       "MobileNo_Avl_Flag                         0\n",
       "Aadhar_flag                               0\n",
       "PAN_flag                                  0\n",
       "VoterID_flag                              0\n",
       "Driving_flag                              0\n",
       "Passport_flag                             0\n",
       "PERFORM_CNS.SCORE                         0\n",
       "PERFORM_CNS.SCORE.DESCRIPTION             0\n",
       "PRI.NO.OF.ACCTS                           0\n",
       "PRI.ACTIVE.ACCTS                          0\n",
       "PRI.OVERDUE.ACCTS                         0\n",
       "PRI.CURRENT.BALANCE                       0\n",
       "PRI.SANCTIONED.AMOUNT                     0\n",
       "PRI.DISBURSED.AMOUNT                      0\n",
       "SEC.NO.OF.ACCTS                           0\n",
       "SEC.ACTIVE.ACCTS                          0\n",
       "SEC.OVERDUE.ACCTS                         0\n",
       "SEC.CURRENT.BALANCE                       0\n",
       "SEC.SANCTIONED.AMOUNT                     0\n",
       "SEC.DISBURSED.AMOUNT                      0\n",
       "PRIMARY.INSTAL.AMT                        0\n",
       "SEC.INSTAL.AMT                            0\n",
       "NEW.ACCTS.IN.LAST.SIX.MONTHS              0\n",
       "DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS       0\n",
       "AVERAGE.ACCT.AGE                          0\n",
       "CREDIT.HISTORY.LENGTH                     0\n",
       "NO.OF_INQUIRIES                           0\n",
       "loan_default                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniqueID                                        int64\n",
       "disbursed_amount                                int64\n",
       "asset_cost                                      int64\n",
       "ltv                                           float64\n",
       "branch_id                                       int64\n",
       "supplier_id                                     int64\n",
       "manufacturer_id                                 int64\n",
       "Current_pincode_ID                              int64\n",
       "Date.of.Birth                          datetime64[ns]\n",
       "Employment.Type                                object\n",
       "DisbursalDate                          datetime64[ns]\n",
       "State_ID                                        int64\n",
       "Employee_code_ID                                int64\n",
       "MobileNo_Avl_Flag                               int64\n",
       "Aadhar_flag                                     int64\n",
       "PAN_flag                                        int64\n",
       "VoterID_flag                                    int64\n",
       "Driving_flag                                    int64\n",
       "Passport_flag                                   int64\n",
       "PERFORM_CNS.SCORE                               int64\n",
       "PERFORM_CNS.SCORE.DESCRIPTION                  object\n",
       "PRI.NO.OF.ACCTS                                 int64\n",
       "PRI.ACTIVE.ACCTS                                int64\n",
       "PRI.OVERDUE.ACCTS                               int64\n",
       "PRI.CURRENT.BALANCE                             int64\n",
       "PRI.SANCTIONED.AMOUNT                           int64\n",
       "PRI.DISBURSED.AMOUNT                            int64\n",
       "SEC.NO.OF.ACCTS                                 int64\n",
       "SEC.ACTIVE.ACCTS                                int64\n",
       "SEC.OVERDUE.ACCTS                               int64\n",
       "SEC.CURRENT.BALANCE                             int64\n",
       "SEC.SANCTIONED.AMOUNT                           int64\n",
       "SEC.DISBURSED.AMOUNT                            int64\n",
       "PRIMARY.INSTAL.AMT                              int64\n",
       "SEC.INSTAL.AMT                                  int64\n",
       "NEW.ACCTS.IN.LAST.SIX.MONTHS                    int64\n",
       "DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS             int64\n",
       "AVERAGE.ACCT.AGE                               object\n",
       "CREDIT.HISTORY.LENGTH                          object\n",
       "NO.OF_INQUIRIES                                 int64\n",
       "loan_default                                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "train['Date.of.Birth'] = pd.to_datetime(train['Date.of.Birth'])\n",
    "train['DisbursalDate'] = pd.to_datetime(train['DisbursalDate'])\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "train['Age'] = ((now - train['Date.of.Birth'])/365).dt.days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = (now - train['DisbursalDate'])\n",
    "train['Days_Since_Disbursal'] = delta.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CREDIT.HISTORY.LENGTH_yr'] = train['CREDIT.HISTORY.LENGTH'].apply(lambda x: x.split(' ')[0])\n",
    "train['CREDIT.HISTORY.LENGTH_yr'] = train['CREDIT.HISTORY.LENGTH_yr'].apply(lambda x: x.split('yrs')[0])\n",
    "\n",
    "train['CREDIT.HISTORY.LENGTH_mon'] = train['CREDIT.HISTORY.LENGTH'].apply(lambda x: x.split(' ')[1])\n",
    "train['CREDIT.HISTORY.LENGTH_mon'] = train['CREDIT.HISTORY.LENGTH_mon'].apply(lambda x: x.split('mon')[0])\n",
    "\n",
    "train['CREDIT.HISTORY.LENGTH_total_months'] = (train['CREDIT.HISTORY.LENGTH_yr'].astype(int))*12 + train['CREDIT.HISTORY.LENGTH_mon'].astype(int)\n",
    "train = train.drop(['CREDIT.HISTORY.LENGTH_yr', 'CREDIT.HISTORY.LENGTH_mon'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['AVERAGE.ACCT.AGE_yr'] = train['AVERAGE.ACCT.AGE'].apply(lambda x: x.split(' ')[0])\n",
    "train['AVERAGE.ACCT.AGE_yr'] = train['AVERAGE.ACCT.AGE_yr'].apply(lambda x: x.split('yrs')[0])\n",
    "\n",
    "train['AVERAGE.ACCT.AGE_mon'] = train['AVERAGE.ACCT.AGE'].apply(lambda x: x.split(' ')[1])\n",
    "train['AVERAGE.ACCT.AGE_mon'] = train['AVERAGE.ACCT.AGE_mon'].apply(lambda x: x.split('mon')[0])\n",
    "\n",
    "train['AVERAGE.ACCT.AGE_total_months'] = (train['AVERAGE.ACCT.AGE_yr'].astype(int))*12 + train['AVERAGE.ACCT.AGE_mon'].astype(int)\n",
    "\n",
    "train = train.drop(['AVERAGE.ACCT.AGE_yr', 'AVERAGE.ACCT.AGE_mon'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    129785\n",
       "5     50728\n",
       "4     18294\n",
       "3     12412\n",
       "2     12025\n",
       "1      9910\n",
       "Name: PERFORM_CNS.SCORE.DESCRIPTION, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Label Encoding for PERFORM_CNS.SCORE.DESCRIPTION\n",
    "\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('No Bureau History Available', 0)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Sufficient History Not Available', 0)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Not Enough Info available on the customer', 0)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Activity seen on the customer (Inactive)',0)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Updates available in last 36 months', 0)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Only a Guarantor', 0)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: More than 50 active Accounts found',0)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('M-Very High Risk', 1)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('L-Very High Risk', 1)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('K-High Risk', 2)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('J-High Risk', 2)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('I-Medium Risk', 3)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('H-Medium Risk', 3)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('G-Low Risk', 4)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('F-Low Risk', 4)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('E-Low Risk', 4)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('D-Very Low Risk', 5)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('C-Very Low Risk', 5)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('B-Very Low Risk', 5)\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].replace('A-Very Low Risk', 5)\n",
    "\n",
    "# checing the values in bureau score\n",
    "train['PERFORM_CNS.SCORE.DESCRIPTION'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['branch_id'] = train['branch_id'].astype('category')\n",
    "train['manufacturer_id'] = train['manufacturer_id'].astype('category')\n",
    "train['State_ID'] = train['State_ID'].astype('category')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['branch_id'] = le.fit_transform(train['branch_id'])\n",
    "train['manufacturer_id'] = le.fit_transform(train['manufacturer_id'])\n",
    "train['State_ID'] = le.fit_transform(train['State_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PRI.CURRENT.BALANCE'].fillna(train['PRI.CURRENT.BALANCE'].mean(), inplace = True)\n",
    "train['PRI.SANCTIONED.AMOUNT'].fillna(train['PRI.SANCTIONED.AMOUNT'].mean(), inplace = True)\n",
    "train['SEC.CURRENT.BALANCE'].fillna(train['SEC.CURRENT.BALANCE'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PERFORM_CNS.SCORE'] = np.log1p(train['PERFORM_CNS.SCORE'])\n",
    "train['disbursed_amount'] = np.log1p(train['disbursed_amount'])\n",
    "train['asset_cost'] = np.log1p(train['asset_cost'])\n",
    "train['ltv'] = np.log1p(train['ltv'])\n",
    "train['PRIMARY.INSTAL.AMT'] = np.log1p(train['PRIMARY.INSTAL.AMT'])\n",
    "train['SEC.INSTAL.AMT'] = np.log1p(train['SEC.INSTAL.AMT'])\n",
    "train['SEC.NO.OF.ACCTS'] = np.log1p(train['SEC.NO.OF.ACCTS'])\n",
    "train['SEC.ACTIVE.ACCTS'] = np.log1p(train['SEC.ACTIVE.ACCTS'])\n",
    "train['SEC.OVERDUE.ACCTS'] = np.log1p(train['SEC.OVERDUE.ACCTS'])\n",
    "train['SEC.SANCTIONED.AMOUNT'] = np.log1p(train['SEC.SANCTIONED.AMOUNT'])\n",
    "train['SEC.DISBURSED.AMOUNT'] = np.log1p(train['SEC.DISBURSED.AMOUNT'])\n",
    "#train['SEC.CURRENT.BALANCE'] = np.log1p(train['SEC.CURRENT.BALANCE'])\n",
    "train['PRI.NO.OF.ACCTS'] = np.log1p(train['PRI.NO.OF.ACCTS'])\n",
    "train['PRI.ACTIVE.ACCTS'] = np.log1p(train['PRI.ACTIVE.ACCTS'])\n",
    "train['PRI.OVERDUE.ACCTS'] = np.log1p(train['PRI.OVERDUE.ACCTS'])\n",
    "#train['PRI.CURRENT.BALANCE'] = np.log1p(train['PRI.CURRENT.BALANCE'])\n",
    "#train['PRI.SANCTIONED.AMOUNT'] = np.log1p(train['PRI.SANCTIONED.AMOUNT'])\n",
    "train['PRI.DISBURSED.AMOUNT'] = np.log1p(train['PRI.DISBURSED.AMOUNT'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9839 112392\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Date.of.Birth - derive Age from this and drop this \n",
    "\n",
    "test['Date.of.Birth'] = pd.to_datetime(test['Date.of.Birth'])\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "test['Age'] = ((now - test['Date.of.Birth'])/365).dt.days\n",
    "\n",
    "sns.distplot(test['Age'],  color = 'orange')\n",
    "\n",
    "print(len(test[test['Age'] <= 0]), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    405\n",
      "1    151\n",
      "2    142\n",
      "Name: Days_Since_Disbursal, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1471b4be0>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAELCAYAAAA1AlaNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XucXWV97/HPLzO5kZBgwiAxFxNMABNBiiOIVLSktMFWojaUUIV4Dm0O1tR6PLYCthTx8hKxpahYiQaNoRVsPNp5SWjgcFNAQgIJlyREhySYG5J7yGVmMpPf+eN5lntlZ09mJ7Mva8/+vl+vee29116z17Nm7/nuZ/3WWs8yd0dEROpDv2o3QEREKkehLyJSRxT6IiJ1RKEvIlJHFPoiInVEoS8iUkcU+iIidUShLyJSRxT6IiJ1pLGYmcxsGnA70AB8192/kvf8QOAHwDuA7cAV7r7ezD4C/F1q1rOBc919RXfLOvnkk338+PHHtBIiIvXumWee2ebuTT3NZz0Nw2BmDcCvgEuAjcBS4Ep3X5Wa56+Bs939WjObCXzI3a/Ie52zgJ+6+1uOtrzm5mZftmxZT+0WEZEUM3vG3Zt7mq+Y8s55QKu7r3X3DuAeYHrePNOB+fH+QmCqmVnePFfG3xURkSopJvRHAxtSjzfGaQXncfdOYDcwMm+eK4AfHl8zRUSkFCqyI9fMzgf2u/uL3Tw/28yWmdmyrVu3VqJJIiJ1qZjQ3wSMTT0eE6cVnMfMGoHhhB26iZkcpZfv7nPdvdndm5uaetwPISIix6mY0F8KTDKzCWY2gBDgLXnztACz4v0ZwMMe9xCbWT/gz1E9X0Sk6no8ZNPdO81sDrCYcMjmXe6+0sxuBpa5ewswD1hgZq3ADsIXQ+IiYIO7ry1980VE5Fj0eMhmpemQTRGRY1fKQzZFRKSPUOiXQGcnXHQRPPRQtVsiInJ0Cv0S2L0bfvELWLq02i0RETk6hX4JdHSE27a26rZDRKQnCv0SaG8Ptwp9Eck6hX4JqKcvIrVCoV8C6umLSK1Q6JeAQl9EaoVCvwRU3hGRWqHQL4Gkp5/ciohklUK/BFTeEZFaodAvAZV3RKRWKPRLQD19EakVCv0SUOiLSK1Q6JeAyjsiUisU+iWgnr6I1AqFfgmopy8itUKhXwI6Tl9EaoVCvwRU3hGRWqHQL4F0eSdjlxwWETmMQr8Ekp6+Oxw8WN22iIgcjUK/BNK1fJV4RCTLigp9M5tmZmvMrNXMrivw/EAzuzc+v8TMxqeeO9vMfmlmK83sBTMbVLrmZ0NS3gGFvohkW4+hb2YNwB3ApcBk4Eozm5w32zXATnefCNwG3BJ/txG4G7jW3acA7wP6XAFEPX0RqRXF9PTPA1rdfa27dwD3ANPz5pkOzI/3FwJTzcyAPwKed/fnANx9u7t3labp2aGevojUimJCfzSwIfV4Y5xWcB537wR2AyOB0wE3s8Vm9qyZ/X2hBZjZbDNbZmbLtm7deqzrUHXq6YtIrSj3jtxG4PeBj8TbD5nZ1PyZ3H2uuze7e3NTU1OZm1R66dDXCVoikmXFhP4mYGzq8Zg4reA8sY4/HNhO2Cr4ubtvc/f9wCLg3N42OmtU3hGRWlFM6C8FJpnZBDMbAMwEWvLmaQFmxfszgIfd3YHFwFlmdkL8MngvsKo0Tc+O9nboF/+SCn0RybLGnmZw904zm0MI8AbgLndfaWY3A8vcvQWYBywws1ZgB+GLAXffaWb/QvjicGCRu99XpnWpmvZ2GDYMdu1S6ItItvUY+gDuvohQmklPuzF1vw24vJvfvZtw2Gaf1dEBw4cr9EUk+3RGbgm0t4fQB4W+iGSbQr8EkvIOKPRFJNsU+iWQlHdAoS8i2abQLwGVd0SkVij0S6CjI1fe0clZIpJlCv0SaG+HE06Ahgb19EUk2xT6JdDeDgMGwKBBCn0RybaijtOXI82dG24PHYLOTnjxxfBYoS8iWaaefi91xYGiGxuhf3+Fvohkm0K/lzo7w61CX0RqgUK/lxT6IlJLFPq9lA79xkaFvohkm0K/l/J7+jpOX0SyTKHfS+rpi0gtUej3kmr6IlJLFPq9pNAXkVqi0O8lhb6I1BKFfi8p9EWklij0e0k7ckWklij0e0k9fRGpJQr9XkpCv6FBPX0Ryb6iQt/MppnZGjNrNbPrCjw/0Mzujc8vMbPxcfp4MztgZiviz7dL2/zqS0K/f//w09mZG4RNRCRrehxa2cwagDuAS4CNwFIza3H3VanZrgF2uvtEM5sJ3AJcEZ972d3PKXG7MyO/vAO5i6qIiGRNMT3984BWd1/r7h3APcD0vHmmA/Pj/YXAVDOz0jUzu9LlnST0VeIRkawqJvRHAxtSjzfGaQXncfdOYDcwMj43wcyWm9ljZvaeXrY3c9Llnca43aTQF5GsKveVs7YA49x9u5m9A/ipmU1x9z3pmcxsNjAbYNy4cWVuUmkVKu8o9EUkq4rp6W8CxqYej4nTCs5jZo3AcGC7u7e7+3YAd38GeBk4PX8B7j7X3ZvdvbmpqenY16KKVN4RkVpSTOgvBSaZ2QQzGwDMBFry5mkBZsX7M4CH3d3NrCnuCMbMTgMmAWtL0/Rs6OyEfv3Cj0JfRLKux/KOu3ea2RxgMdAA3OXuK83sZmCZu7cA84AFZtYK7CB8MQBcBNxsZgeBQ8C17r6jHCtSLZ2dubBXTV9Esq6omr67LwIW5U27MXW/Dbi8wO/9GPhxL9uYaZ2dobQD6umLSPbpjNxe6uzM9fDTx+mLiGSRQr+X0qGv8o6IZJ1Cv5e6uo7s6Sv0RSSrFPq9dPCgQl9EaodCv5cK1fQV+iKSVQr9XlJ5R0RqiUK/l9LlHe3IFZGsU+j3Urqnr9AXkaxT6PdSuqZvBgMHKvRFJLsU+r2UDn2AQYN0cpaIZJdCv5cKhb56+iKSVQr9InV2wt13H9mLV+iLSC1R6BfpySfhqqvg1lsPn67QF5FaotAv0s6d4fYrX4FXX81NV+iLSC1R6BdpT7zA4759cNNNuen5oa+jd0QkyxT6RUpC/8or4Tvfgc2bw+P0ePqgnr6IZJtCv0hJ6N96KwwdCg8+GE7Mcs8NvwAKfRHJNoV+kfbsgQEDYPRoOP/80NNPLoqu4/RFpFYo9Iu0Zw8MGxbuT5oEr72WC/388s6BA5Vvn4hIMRT6KY89Bu95D3R0HPlcOvQnToT9+2H37vA4Xd4ZMKDw74uIZIFCP+Wpp+Dxx0MvPl9+Tx9yO3Pzj95ReUdEskqhn7J3b7jdtevI5/J7+pAL/XR5R6EvIllWVOib2TQzW2NmrWZ2XYHnB5rZvfH5JWY2Pu/5cWa218w+U5pml0exoT9hQhhRc8uW8Dhd3lHoi0iW9Rj6ZtYA3AFcCkwGrjSzyXmzXQPsdPeJwG3ALXnP/wtwf++bW17Fhv7AgTBiBGzaFB6nyzuq6YtIlhXT0z8PaHX3te7eAdwDTM+bZzowP95fCEw1MwMwsw8C64CVpWly+RQb+gCnnAJbt4b7+eWdjo5w/L6ISNYUE/qjgQ2pxxvjtILzuHsnsBsYaWZDgc8Cn+99U8vvWEP/0KFwP7+8A+rti0g2lXtH7k3Abe6+92gzmdlsM1tmZsu2Jt3nKnj99XCbH/odHeEs2/zQT+T39EF1fRHJpsaeZ2ETMDb1eEycVmiejWbWCAwHtgPnAzPM7KvAScAhM2tz92+mf9nd5wJzAZqbm6tSGJk7F9atC/d//vPwGGD27NyXQXehn1/TB/X0RSSbign9pcAkM5tACPeZwF/kzdMCzAJ+CcwAHnZ3B96TzGBmNwF78wM/S5Le+f79h09Pxt3pLvQLlXfU0xeRLOox9N2908zmAIuBBuAud19pZjcDy9y9BZgHLDCzVmAH4Yuh5iRBnT+MQqHQHzkyHLbprvKOiNSOYnr6uPsiYFHetBtT99uAy3t4jZuOo30VdSw9/f79w2Gb27cfeUZu+rVERLJEZ+RG7scW+pAr8eSPvQOq6YtINhXV068HnZ25QzDToT93Ljz9dLh///2wfHnuuVNOgdWrVd4Rkdqhnn6UhLTZkTX95KIogwcfPv2002DIkDCcckKhLyJZpp5+lAT78OFhyGT38AUAuS+BdLhDuJjKO995eE8/Ke8o9EUki9TTj5KQPumkw+v7EL4QzHKBnjA7PPBBZ+SKSLYp9KMk5N/whnCbruu3tYXSTtLzPxqVd0QkyxT6UX7op+v6Bw4cWdrpjkJfRLJMoR+lyztwZE+/2NDXIZsikmUK/SjZkTtiRLgtVN4phnr6IpJlCv1I5R0RqQcK/ahU5R2FvohkmUI/yg/9dE//WMo7qumLSJYp9KP29tBLb2wMt+me/rGUd3RylohkmUI/amvLlWYGD8719A8dCgFebOj36xcGYFPoi0gWKfSjpKcPcMIJuZ5+Et7FlncgvI5CX0SySKEfpUN/8OBc6Hc37s7RDBigmr6IZJNCP0qXd044IRf23Y2weTTq6YtIVin0o556+gp9EekLFPpRemdtoZ7+sZR3FPoiklUK/ajQjlz34wt91fRFJKsU+lF+6Cdj6qu8IyJ9SVGhb2bTzGyNmbWa2XUFnh9oZvfG55eY2fg4/TwzWxF/njOzD5W2+aWTX9OH0NtXeUdE+pIeQ9/MGoA7gEuBycCVZjY5b7ZrgJ3uPhG4DbglTn8RaHb3c4BpwJ1mlrlLNHZ0hAujp3v6EHr5SU8/ea4YAwYo9EUkm4rp6Z8HtLr7WnfvAO4BpufNMx2YH+8vBKaambn7fnfvjNMHAV6KRpfavn3htrue/qBB4UzbYg0cqJq+iGRTMVE2GtiQerwxTis4Twz53cBIADM738xWAi8A16a+BDJj795wmz56B0Lo7959bKUdUHlHRLKr7Dty3X2Ju08B3glcb2ZHRKiZzTazZWa2bOvWreVu0hGS0M8v79x3HyxdCmeeeWyvp9AXkawqJvQ3AWNTj8fEaQXniTX74cD29AzuvhrYC7wtfwHuPtfdm929uampqfjWl0h+6CflnVdegYsugquvPrbXU01fRLKqmJ2qS4FJZjaBEO4zgb/Im6cFmAX8EpgBPOzuHn9ng7t3mtmbgTOB9aVqfKkUKu+89a1w1llw8cVgdmyvp5q+iGRVj6EfA3sOsBhoAO5y95VmdjOwzN1bgHnAAjNrBXYQvhgAfh+4zswOAoeAv3b3beVYkd7I7+n36wef+tTxv57KOyKSVUUdPunui4BFedNuTN1vAy4v8HsLgAW9bGPZ5Yd+byn0RSSrdEYupQ99DcMgIlml0OfImn5vJTV9z+RZCSJSzxT6wOuvh9vk+ra9lWwxqLcvIlmj0Cf09Pv3h4aG0rxeEvqq64tI1ij0CaFfqno+5LYY1NMXkaxR6FP60FdPX0SySqGPQl9E6odCH4W+iNQPhT4h9Et1uCaopi8i2aXQJ4R+qQ7XBPX0RSS7FPqEq2Mp9EWkHij0CVfH6t+/dK+n0BeRrFLoU/rQV01fRLJKoU8I/cYSXq5dPX0RySqFPirviEj9qPvQ7+qCzs7ylHcU+iKSNXUf+kkwl6Onr5q+iGRN3Yd+W1u4VXlHROqBQj+Gvnbkikg9UOiXoaevmr6IZJVCv4yhr5q+iGSNQr8Mod+vX3g99fRFJGuKCn0zm2Zma8ys1cyuK/D8QDO7Nz6/xMzGx+mXmNkzZvZCvL24tM3vvXKEPoS6vkJfRLKmx9A3swbgDuBSYDJwpZlNzpvtGmCnu08EbgNuidO3AR9w97OAWcCCUjW8VMqxIxdCiUflHRHJmmJ6+ucBre6+1t07gHuA6XnzTAfmx/sLgalmZu6+3N03x+krgcFmVsLLlfSeevoiUk+KCf3RwIbU441xWsF53L0T2A2MzJvnz4Bn3T1TUViunr5CX0SyqMRRV5iZTSGUfP6om+dnA7MBxo0bV4km/Y56+iJST4rp6W8CxqYej4nTCs5jZo3AcGB7fDwG+Alwtbu/XGgB7j7X3ZvdvbmpqenY1qCXyhX6qumLSBYVE/pLgUlmNsHMBgAzgZa8eVoIO2oBZgAPu7ub2UnAfcB17v5EqRpdSurpi0g96TH0Y41+DrAYWA38yN1XmtnNZnZZnG0eMNLMWoFPA8lhnXOAicCNZrYi/pxS8rXoBYW+iNSTomr67r4IWJQ37cbU/Tbg8gK/90Xgi71sY1lpR66I1BOdkdvHj9N/+WVYvbrarRCRrKjI0TtZ1tYGgwaBWWlfNys9/U9/GrZuhSefrHZLRCQL1NOPoV9qWQn97dthx45qt0JEskKhX6bQHzAgG6G/bx/s3VvtVohIVqi8U+LQnzs33K5bB7t25R7Pnl26ZRyLvXtD8IuIgHr6tLeXp6ff2AgHD5b+dY+VQl9E0uo+9MtV3mlshM7O0r/usdq7N3z5ZOFIIhGpPoV+Hw5991wvX719EQGFfllDv6srBG+1HDiQW75CX0RAoV/W0Ifq9vbTR+0o9EUEFPplC/1kLB+FvohkiUK/TKHf0BBuqxn66aBX6IsIKPRpawtnz5Za1so7OkFLREChr/KOiNQVhb525IpIHVHoq6YvInWkrkP/0KFwpmo9lHdU0xcRqPPQT0bBLGd5p5rj76i8IyL56jr0k6tmlSP0k9es5vDKSegPGqTQF5GgrodWLmfon3BCuN2/v/SvXay9e0M7hg5V6ItIoJ4+5Qn9wYPDbTVDf98+GDIk/KimLyKg0Af6dk9/6NAQ+urpiwgUGfpmNs3M1phZq5ldV+D5gWZ2b3x+iZmNj9NHmtkjZrbXzL5Z2qb3XjlDv7ExXDLxwIHSv3axFPoikq/H0DezBuAO4FJgMnClmU3Om+0aYKe7TwRuA26J09uAfwQ+U7IWl1A5Qx9Cb189fRHJkmJ6+ucBre6+1t07gHuA6XnzTAfmx/sLgalmZu6+z90fJ4R/5pQ79AcPrm5PP6npDx2qmr6IBMWE/mhgQ+rxxjit4Dzu3gnsBkYW2wgzm21my8xs2datW4v9tV4r53H6oJ6+iGRPJnbkuvtcd2929+ampqaKLbcSPX2FvohkSTGhvwkYm3o8Jk4rOI+ZNQLDge2laGA5VaKmrx25IpIlxYT+UmCSmU0wswHATKAlb54WYFa8PwN42L2aV4ctTl/v6e/bF0I/qeln/x0RkXLr8Yxcd+80sznAYqABuMvdV5rZzcAyd28B5gELzKwV2EH4YgDAzNYDw4ABZvZB4I/cfVXpV+XYVaqnX42w7eoKXzhDhoSLxBw6FPZhlGtdRaQ2FDUMg7svAhblTbsxdb8NuLyb3x3fi/aVVRL65bhyFoSefhK2lZZsYQwdGs4XgNDzr2Tou+uLRuRoXn8drr8evvxlGDasMsvs02PvzJ1bePrs2eG2Ej19qE5dPzlEc+jQ3DDP+/bByKKPqeq9W26Bb3wDXnoJTjyxcssVqRWPPAJ33AGXXALT8w+EL5NMHL1TLeXu6VdzKIZkx21S009Pq5QXXoDNm+Hb367sckVqxfr14fbVVyu3zD7d0+9JW1soffQr01dfMuhaNXv6Tz6ZW78FC2D8+HA/2doppy1bwu3XvgZz5uT+HiISJKGf/K9UQt339MtZb65mTz8J/QEDclsyld63sGULvPnN8Npr8J3vVHbZIrVg3bpwq9CvkHKHfjWHV05Cf+DA3I7cjo7KtmHzZrjsMrjoIvjqV6t7QRmRLKpGeUehX4GefjXKO0n9ftCg6vT09++HPXvgTW+CG26ATZvgZz+r3PJFakE1yjt1X9Ovh56+WbhfydBPPsSjRsGFF4b7v/515ZYvknW7doUfUHmnYsod+tUcU7/aNf106A8dGg4VfeWVyi1fJOuSXv7EiaG8c+hQZZar0C/ziUPVGmkzfVH0atT0f/CDcPvEE+F8iSFD4Be/qNzyJbsOHYLVq6vdiupLQv+CC6CzE3bsqMxyFfoVCP1q1fTNwolZ/fuH+5Xs6e/eHW6HDw+3J58M2zM/BJ9Uwk9+AlOmqNyXDn2oXIlHoV/m0K/WoGvJCJtm4WfgwMqHfmNj6OEDjBgRQl+Dvsny5eFzsGJFtVtSXevXh//RKVPCY4V+BfT18k5yJi6EEk+lQ3/48NxO5JEj4eBBqOA1cqSAG24IPe1qeumlcLsqE8MuVs/69eFkyVGjwuNKHbap0O/DPf2klw2hp1/Jmv7u3YcPIJWM+ZNs0krltbWF8yXuvLO67VDoB+vWwYQJudCvVE9fh2z24Zp+uqdfjfLOKafkHqdD/7zzKtcOCLXjYcPgjW+s7HKzZtWqMOT2c89Vrw1dXblafr3tzE0PAOkOv/pV+L9IxsdSeacCKjHsb9LTr3QtO7+8U+nQ37UrtxMXqtfT37cP3v720Ju64AL40Y8qu/wsSWror74Kv/1tddqwfn3Y4mxqgjVrwlEr9Wj//tDpTP4vRo1SeaciKtXTd88dQlkp1azpt7WFD/VJJ+WmDR4c/haVPlZ/yZKwpfWRj4RD4mbNqt/hINI9/Gr19pPSzvTpIfyTsWfqTXIkWxL6p56qnn5FVCr0IXfmXaUU6ulXqqaf9FjSPX0IH/BK9/SfeCLsTP7GN+DWW8N7/vTTlW1DVqxYAaefHu5XO/Q/9KFwW691/W3bwu3JJ4fbUaMU+mXnXrkduVD50N+378gduZXq4SYf3qyE/pQpoazz8svhC+BrXwv11e4ustMXuYegv/hiGDu2eodLvvRSCLpkaI56Df38nr5CvwKSXm+5LqCSyFJPv1Khv3lzuC0U+q+8Urn9G11d8Mtf5gJmyBAYPTrsQKs3v/lN2Ln+9reHn2r19NesgTPPDJ+N0aPrb2duYtu20OFM8uHUU8P/bCXKwHV79E65L5WYqFZPv1BNv1LlnaP19PftC72cZLO2nFatCiN9Xnhh7giq008Pw0F0doaTx+pF0rM/5xzYuBHuv78yW7pw+BbVihXhS2fuXJg8uT57+h0d8OyzcNppufNY0sfqT5xY3uXXbU+/UqFfyZ5+ayvcfnvYYdneXrim39UV/uHKeVGTLVvC1brSy4fKH8HzxBPhNunpQwj9gwfr73yB554LAbN0aThBrqsLvvCFypa49u4NFwI/9dTw+K1vDeWeSg00lhWPPho6I3/yJ7lplTxWv6jQN7NpZrbGzFrN7LoCzw80s3vj80vMbHzquevj9DVm9sela3rv9LXQ7+iAD38YPvUpeMtbwrT8mr47PPQQPPMMfPKT5Qu+zZvDcfH5l6GsRuifemo4ASYxaVK4rbcSz4oV4byJgQNhzJgwbePGyrYhOUw0OV9i8uSw5bdhQ2XbUU1tbbB4cVj3dI++kmfl9hj6ZtYA3AFcCkwGrjSzyXmzXQPsdPeJwG3ALfF3JwMzgSnANOBb8fVKbv9++Ku/6vmDvHQpfPnL4ZsWKlveeeQR+Oxnc4ORQeh1Jl9AR7N27eHz3XdfuCLV44+Hx7fcEi5E/tWv5k5+Gjs2N38y0mZLSwjBfv3g058O01auhGnTYOHC41vHfFu2HH64ZiIJ/UodtvnEE6GXn2xCQ9j6eNObqjPYV0dH2AKr9BXMIPT0k7BvagrhX+mwTQIt3dOH+qrrP/JI2OK57LLDpyd/k6z09M8DWt19rbt3APcA0/PmmQ7Mj/cXAlPNzOL0e9y93d3XAa3x9Upu+XL44Q/hrLPCkRpPPw3/9m/wd38XxhrZtg3mz4fvfhc+9zn4wAfC75U79Bsawj/Y978PU6eGUP6934Onngqnw48bF3pgX/xi6Al95Svh8ZQpcO+9oUf6gQ+E3vuUKSG0v/a1MO3JJ8PRGJ//fNhUv+KKsL6LF4dg/fCHc+1Idlh3dcHVV8M//EP4u1x/PbzrXfDAA3D55XDjjWGT++MfhzPOCGO1bNkSemSPPRa+GPJ3xnZ1hd954IHw3JYtR9bzIWz1DBsW1mnZsvBaa9bkNu/374edO49/R697+ELduze0Yd26w0s7idNPD0fydHaGv/m2beXdufz44/DHfxzeg+RKZpdcEgKgmOV2dh4+3/79YWsp2U/R1hbOR2hpCTtsDx0K78WHPwyXXgp33x06DUno9+sXdqKuWwcPPwzvfS985jPhi7C9PXxZzpsX3qPOzjDt2WfhwQdzvfXdu0P7H3oo956tWAG33QYLFoT5fvOb8Dn6m7+Bb34Tnn8+7EdJvvwnx67jDTdAc3P4ufPOsH5bt4Yrrd13X+7wxra28HnZvDn39zh0KJQz0yd5uYfOVL7jfY+7uo58P3bvPrwNmzaFz1Sy3FdfDe/H4sXh77NjR1iX++8PGZXe+oTwN2lsrEzoF7MrazSQ7hNsBM7vbh537zSz3cDIOP2pvN8dfdytPYoLLwwfuo9+NIQfhJCZMCH84f/7v0OP7/3vh499LGwVQGV2ZA0eHP7prr4arroKrrkmN5zqhReGntc//mP4gRAQGzbAzJnh8dChIZz/67/CSS0AM2bAv/5rWJebbgofmq9/PbfMceMOb0MS+lOnhp7urFnwve+FL5nm5vAF86UvhS+PL3whzP/Od4bnb701fMDTH/5TTgkB7h4+qPnjC110UeG/xfjxRx4umQzMlpTABg4Mr9/eHrbIzMKyknKVe+4HQoi5hwuwJ+1I1re70H/00VDi+sQnwrTBg8P7cOBA+ILr3x9OPPHwK48dj507w47rE08M7+vgwSG8li8PX9ijRuWWkZzU1q9f+Ow2NISw2Ls3bKmNGBECJwlBCG3eufPw0EsG+WtqCsu76qowPb3lN3Zs+BJfvz4cTXP77fDP/3zkUV5DhoQtk3SIjhhx5NjvJ54Y6vVpDQ1hXc4+O3QK9u0L69sQt/VPPhne/e4QmGecEYL+2mtDiTJ/67ep6fAv5xNOCJ+b114Ln0uzME+/fqFtHR1hnhEjQtt37Qq3J50U/p8OHAjB3a9f+GwNGhTav2dP+BsMGxbauXNn+FsOHgxveEN4jaQdAwaEZW7fnmtvv37hfzF/YMHkQIq3vS33f53Wr1/lTtAy7+Hrz8xmANPc/S/j46uA8919TmqeF+M8G+PjlwlfDDcBT7n73XH6POB+d1+Yt4zZwOz48AxgTe9XrWxOBrb1OFft6avrBX133bRetaXc6/Vmd2/qaaYncJNdAAAIv0lEQVRievqbgFQfgTFxWqF5NppZIzAc2F7k7+Luc4GaOFXGzJa5e3O121FqfXW9oO+um9artmRlvYqp6S8FJpnZBDMbQNgx25I3TwswK96fATzsYROiBZgZj+6ZAEwC6vQkeBGR6uuxpx9r9HOAxUADcJe7rzSzm4Fl7t4CzAMWmFkrsIPwxUCc70fAKqAT+IS7dxVckIiIlF1R5yS6+yJgUd60G1P324DLu/ndLwFf6kUbs6YmylDHoa+uF/TdddN61ZZMrFePO3JFRKTvqNthGERE6pFCP4+Z3WVmr8XDUJNpN5nZJjNbEX/en3ouk8NM5DOzsWb2iJmtMrOVZva3cfoIM3vQzH4db98Qp5uZfT2u2/Nmdm5116Cwo6xXTb9nZjbIzJ42s+fien0+Tp8QhzppjUOfDIjTux0KJWuOsm7fN7N1qffsnDi9Jj6LCTNrMLPlZvaz+Dhb75m76yf1A1wEnAu8mJp2E/CZAvNOBp4DBgITgJeBhmqvQzfrNQo4N94/EfhVbP9Xgevi9OuAW+L99wP3Awa8C1hS7XU4xvWq6fcs/t2Hxvv9gSXxffgRMDNO/zbw8Xj/r4Fvx/szgXurvQ7HsW7fB2YUmL8mPoup9n4a+A/gZ/Fxpt4z9fTzuPvPCUcgFaNiw0z0lrtvcfdn4/3XgdWEs6PTQ2jMBz4Y708HfuDBU8BJZjaqws3u0VHWqzs18Z7Fv3syunr/+OPAxYShTuDI96vQUCiZc5R1605NfBYBzGwM8CfAd+NjI2PvmUK/eHPipuVdSQmEwkNUlGWYiVKKm5G/R+hhvdHdk5O/XwXiGIi1t2556wU1/p7FMsEK4DXgQcJWyS53TwZdSLf9sKFQgGQolEzKXzd3T96zL8X37DYzSy5xVDPvGfCvwN8DyYDRI8nYe6bQL86/AW8BzgG2AP9c3eYcPzMbCvwY+JS770k/52E7syYP5yqwXjX/nrl7l7ufQziT/TzgzCo3qWTy183M3gZcT1jHdwIjgM9WsYnHzMz+FHjN3Z+pdluORqFfBHf/bfyQHgK+Q64cUNQwE1lhZv0Jwfjv7v5/4+TfJpvK8fa1OL1m1q3QevWV9wzA3XcBjwAXEEobyfk16bb/br3s8KFQMi21btNiqc7dvR34HrX3nl0IXGZm6wmjEV8M3E7G3jOFfhHy6ocfApIje2pmmIlYK5wHrHb3f0k9lR5CYxbwX6npV8cjJ94F7E6VgTKju/Wq9ffMzJrM7KR4fzBwCWF/xSOEoU7gyPer0FAomdPNur2U6nwYoe6dfs8y/1l09+vdfYy7jyfsmH3Y3T9C1t6zSuwtrqUf4IeEcsBBQv3tGmAB8ALwfHyjRqXm/xyh1roGuLTa7T/Kev0+oXTzPLAi/ryfUEN8CPg18P+AEXF+I1w85+W47s3VXodjXK+afs+As4Hlsf0vAjfG6acRvqRagf8EBsbpg+Lj1vj8adVeh+NYt4fje/YicDe5I3xq4rOYt47vI3f0TqbeM52RKyJSR1TeERGpIwp9EZE6otAXEakjCn0RkTqi0BcRqSMKfRGROqLQl7Iws644PO7KOITu/zGzsn/ezOxzcZnPx+WfH6d/18wmV2D5683shfizysy+aGaD4nNvMrOFR/nd8ZYa0rsSYntPruQypbqKulyiyHE44GFsFczsFMJQs8OAfyrXAs3sAuBPCUMtt8cwGwDg7n9ZruUW8Afuvi2OBzQXuBOY5e6byZ2ZWXJm1ui5gb1EClJPX8rO3V8DZhNGvbTYo/2FmT0bf94NYGY/MLNk2FnM7N/NbLqZTbFw0Y0VsQc/qZtFjQK2eRi7BXffFoMWM3vUzJrj/b1m9qW4BfKUmb0xTn+jmf0kTn8u1a6PppZ/p5k1FLnee4FrgQ9auFjN73ryR1mnxrjeq81soZmdEOf/XY/czJrN7NF4/yYzW2BmTwALuntdM/upmT0Tt4JmF9N+6ZsU+lIR7r4WaABOIQzqdom7nwtcAXw9zjYP+BiAmQ0H3g3cRwjO2+OWQzNheIxCHgDGmtmvzOxbZvbebuYbAjzl7m8Hfg78VZz+deCxOP1cYKWZvTW28cK4/C7gI8ew3nuAdYQxftK6W6czgG+5+1uBPYQLbfRkMvCH7n7lUV73f7r7O+K0T5pZZoddlvJS6Es19Ae+Y2YvEMYemQzg7o8Bk8ysCbgS+HEsV/wSuMHMPgu82d0PFHrR2LN+B2GrYitwr5l9rMCsHcDP4v1ngPHx/sWEIZnxMELnbmBqfM2lFsZ/n0oYS+VYFLowRnfrtMHdn4j37yaMLdSTltTvd/e6nzSz54CnCCM7dre1JH2cQl8qwsxOI/SSXwP+N/Bb4O2EnueA1Kw/AD4K/A/gLgB3/w/gMuAAsMjMLu5uOTGsH3X3fwLmAH9WYLaDnht0qouj79syYL67nxN/znD3m3pa39/9stmJhC+VX+W1s7t1yh8MK3ncSe7/dVDePPuO9rpm9j7gD4EL4lbM8gKvIXVCoS9lF3vu3wa+GcN2OLDFw1j3VxHKPonvA58CcPdV8fdPA9a6+9cJw9Ke3c1yzsir958DvHIMTX0I+Hh8rYZYYnoImBF3RicXkn9zMS8Wd+R+C/ipu+/Me667dRoXd0gD/AXweLy/nrDFAYW/yI72usOBne6+38zOJFxnVuqUQl/KZXByyCZhyOYHgM/H574FzIrlhjM5vKf6W8K48d9LvdafAy/G8srbCFsDhQwF5sdDJZ8nd4H0Yv0t8Aex7PQMMDl+8fwD8EB8zQcJO4yP5pG4w/Zp4DfA/yowT3frtAb4hJmtBt5ALDcR/na3m9kywtZJdwq97n8TdhCvBr5CKPFIndLQypIp8WiVFwiHXe6udntE+hr19CUzzOwPCb38byjwRcpDPX2pOfFww4cKPDXV3StyXVgzWwIMzJt8lbu/UInlixwvhb6ISB1ReUdEpI4o9EVE6ohCX0Skjij0RUTqiEJfRKSO/H83xnULI++TbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##DisbursalDate - derive days elapsed since loan disbursal\n",
    "\n",
    "test['DisbursalDate'] = pd.to_datetime(test['DisbursalDate'])\n",
    "delta = (now - test['DisbursalDate'])\n",
    "test['Days_Since_Disbursal'] = delta.dt.days\n",
    "\n",
    "print(test['Days_Since_Disbursal'][:3])\n",
    "\n",
    "sns.distplot(test['Days_Since_Disbursal'], color='blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    63017\n",
       "5    24993\n",
       "4     8070\n",
       "3     5990\n",
       "2     5642\n",
       "1     4680\n",
       "Name: PERFORM_CNS.SCORE.DESCRIPTION, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##PERFORM_CNS.SCORE.DESCRIPTION - label encode categorical data\n",
    "\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'].value_counts()\n",
    "\n",
    "## Label Encoding for PERFORM_CNS.SCORE.DESCRIPTION\n",
    "\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('No Bureau History Available', 0)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Sufficient History Not Available', 0)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Not Enough Info available on the customer', 0)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Activity seen on the customer (Inactive)',0)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Updates available in last 36 months', 0)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Only a Guarantor', 0)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: More than 50 active Accounts found',0)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('M-Very High Risk', 1)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('L-Very High Risk', 1)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('K-High Risk', 2)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('J-High Risk', 2)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('I-Medium Risk', 3)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('H-Medium Risk', 3)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('G-Low Risk', 4)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('F-Low Risk', 4)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('E-Low Risk', 4)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('D-Very Low Risk', 5)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('C-Very Low Risk', 5)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('B-Very Low Risk', 5)\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'] = test['PERFORM_CNS.SCORE.DESCRIPTION'].replace('A-Very Low Risk', 5)\n",
    "\n",
    "# checing the values in bureau score\n",
    "test['PERFORM_CNS.SCORE.DESCRIPTION'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "##AVERAGE.ACCT.AGE - extract the #yrs and #months as total # months\n",
    "\n",
    "test['AVERAGE.ACCT.AGE_yr'] = test['AVERAGE.ACCT.AGE'].apply(lambda x: x.split(' ')[0])\n",
    "test['AVERAGE.ACCT.AGE_yr'] = test['AVERAGE.ACCT.AGE_yr'].apply(lambda x: x.split('yrs')[0])\n",
    "\n",
    "test['AVERAGE.ACCT.AGE_mon'] = test['AVERAGE.ACCT.AGE'].apply(lambda x: x.split(' ')[1])\n",
    "test['AVERAGE.ACCT.AGE_mon'] = test['AVERAGE.ACCT.AGE_mon'].apply(lambda x: x.split('mon')[0])\n",
    "\n",
    "test['AVERAGE.ACCT.AGE_total_months'] = (test['AVERAGE.ACCT.AGE_yr'].astype(int))*12 + test['AVERAGE.ACCT.AGE_mon'].astype(int)\n",
    "\n",
    "test = test.drop(['AVERAGE.ACCT.AGE_yr', 'AVERAGE.ACCT.AGE_mon'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREDIT.HISTORY.LENGTH  - extract the #yrs and #months as total # months\n",
    "\n",
    "test['CREDIT.HISTORY.LENGTH_yr'] = test['CREDIT.HISTORY.LENGTH'].apply(lambda x: x.split(' ')[0])\n",
    "test['CREDIT.HISTORY.LENGTH_yr'] = test['CREDIT.HISTORY.LENGTH_yr'].apply(lambda x: x.split('yrs')[0])\n",
    "\n",
    "test['CREDIT.HISTORY.LENGTH_mon'] = test['CREDIT.HISTORY.LENGTH'].apply(lambda x: x.split(' ')[1])\n",
    "test['CREDIT.HISTORY.LENGTH_mon'] = test['CREDIT.HISTORY.LENGTH_mon'].apply(lambda x: x.split('mon')[0])\n",
    "\n",
    "test['CREDIT.HISTORY.LENGTH_total_months'] = (test['CREDIT.HISTORY.LENGTH_yr'].astype(int))*12 + test['CREDIT.HISTORY.LENGTH_mon'].astype(int)\n",
    "test = test.drop(['CREDIT.HISTORY.LENGTH_yr', 'CREDIT.HISTORY.LENGTH_mon'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "##branch_id  - convert to category and label encode\n",
    "test['branch_id'] = test['branch_id'].astype('category')\n",
    "le = LabelEncoder()\n",
    "test['branch_id'] = le.fit_transform(test['branch_id'])\n",
    "\n",
    "##manufacturer_id  - convert to category and label encode\n",
    "test['manufacturer_id'] = test['manufacturer_id'].astype('category')\n",
    "test['manufacturer_id'] = le.fit_transform(test['manufacturer_id'])\n",
    "\n",
    "##State_ID - convert to category and label encode\n",
    "test['State_ID'] = test['State_ID'].astype('category')\n",
    "test['State_ID'] = le.fit_transform(test['State_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test['PRI.CURRENT.BALANCE'].fillna(test['PRI.CURRENT.BALANCE'].mean(), inplace = True)\n",
    "test['PRI.SANCTIONED.AMOUNT'].fillna(test['PRI.SANCTIONED.AMOUNT'].mean(), inplace = True)\n",
    "test['SEC.CURRENT.BALANCE'].fillna(test['SEC.CURRENT.BALANCE'].mean(), inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log transform:\n",
    "'''\n",
    "PERFORM_CNS.SCORE\n",
    "disbursed_amount\n",
    "asset_cost\n",
    "ltv\n",
    "PRIMARY.INSTAL.AMT\n",
    "SEC.INSTAL.AMT\n",
    "SEC.NO.OF.ACCTS\n",
    "SEC.ACTIVE.ACCTS\n",
    "SEC.OVERDUE.ACCTS\n",
    "SEC.SANCTIONED.AMOUNT\n",
    "SEC.DISBURSED.AMOUNT\n",
    "SEC.CURRENT.BALANCE\n",
    "PRI.NO.OF.ACCTS\n",
    "PRI.ACTIVE.ACCTS\n",
    "PRI.OVERDUE.ACCTS\n",
    "PRI.CURRENT.BALANCE\n",
    "PRI.SANCTIONED.AMOUNT\n",
    "PRI.DISBURSED.AMOUNT\n",
    "'''\n",
    "\n",
    "test['PERFORM_CNS.SCORE'] = np.log1p(test['PERFORM_CNS.SCORE'])\n",
    "test['disbursed_amount'] = np.log1p(test['disbursed_amount'])\n",
    "test['asset_cost'] = np.log1p(test['asset_cost'])\n",
    "test['ltv'] = np.log1p(test['ltv'])\n",
    "test['PRIMARY.INSTAL.AMT'] = np.log1p(test['PRIMARY.INSTAL.AMT'])\n",
    "test['SEC.INSTAL.AMT'] = np.log1p(test['SEC.INSTAL.AMT'])\n",
    "test['SEC.NO.OF.ACCTS'] = np.log1p(test['SEC.NO.OF.ACCTS'])\n",
    "test['SEC.ACTIVE.ACCTS'] = np.log1p(test['SEC.ACTIVE.ACCTS'])\n",
    "test['SEC.OVERDUE.ACCTS'] = np.log1p(test['SEC.OVERDUE.ACCTS'])\n",
    "test['SEC.SANCTIONED.AMOUNT'] = np.log1p(test['SEC.SANCTIONED.AMOUNT'])\n",
    "test['SEC.DISBURSED.AMOUNT'] = np.log1p(test['SEC.DISBURSED.AMOUNT'])\n",
    "#test['SEC.CURRENT.BALANCE'] = np.log1p(test['SEC.CURRENT.BALANCE'])\n",
    "test['PRI.NO.OF.ACCTS'] = np.log1p(test['PRI.NO.OF.ACCTS'])\n",
    "test['PRI.ACTIVE.ACCTS'] = np.log1p(test['PRI.ACTIVE.ACCTS'])\n",
    "test['PRI.OVERDUE.ACCTS'] = np.log1p(test['PRI.OVERDUE.ACCTS'])\n",
    "#test['PRI.CURRENT.BALANCE'] = np.log1p(test['PRI.CURRENT.BALANCE'])\n",
    "#test['PRI.SANCTIONED.AMOUNT'] = np.log1p(test['PRI.SANCTIONED.AMOUNT'])\n",
    "test['PRI.DISBURSED.AMOUNT'] = np.log1p(test['PRI.DISBURSED.AMOUNT'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15e503710>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.loc[train['Age'] <= 0, 'Age'] = 0\n",
    "test.loc[test['Age'] <= 0, 'Age'] = 0\n",
    "\n",
    "sns.distplot(train['Age'], color='orange')\n",
    "sns.distplot(test['Age'], color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Impute Employment.Type\n",
    "train['Employment.Type'] = train['Employment.Type'].replace(('Self employed', 'Salaried'), (0, 1))\n",
    "\n",
    "emp = train.drop(['UniqueID', 'supplier_id', 'Current_pincode_ID', 'Employee_code_ID', 'Date.of.Birth', 'DisbursalDate', 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into train and test sets\n",
    "train_emp = emp[emp['Employment.Type'].notnull()]\n",
    "y_train_emp = train_emp['Employment.Type'].astype(int)\n",
    "\n",
    "train_emp_ld = train_emp['loan_default'].astype(int)\n",
    "train_emp = train_emp.drop(['Employment.Type', 'loan_default'], axis=1)\n",
    "\n",
    "test_emp = emp[emp['Employment.Type'].isnull()]\n",
    "test_emp_ld = test_emp['loan_default'].astype(int)\n",
    "test_emp = test_emp.drop(['Employment.Type', 'loan_default'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_emp = sc.fit_transform(train_emp)\n",
    "test_emp = sc.transform(test_emp)\n",
    "\n",
    "train_emp = pd.DataFrame(train_emp)\n",
    "test_emp = pd.DataFrame(test_emp)\n",
    "\n",
    "train_emp['loan_default'] = train_emp_ld.values\n",
    "test_emp['loan_default'] = test_emp_ld.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model_emp = LGBMClassifier()\n",
    "\n",
    "model_emp.fit(train_emp, y_train_emp)\n",
    "\n",
    "y_test_emp_pred = model_emp.predict(test_emp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_emp['Employment.Type'] = y_train_emp.values\n",
    "test_emp['Employment.Type'] = y_test_emp_pred\n",
    "\n",
    "train_emp_imputed = pd.concat([train_emp, test_emp], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model based Imputation for the Employment.Type column for the test data set\n",
    "test['Employment.Type'] = test['Employment.Type'].replace(('Self employed', 'Salaried'), (0, 1))\n",
    "\n",
    "emp = test.drop(['supplier_id', 'Current_pincode_ID', 'Employee_code_ID', 'Date.of.Birth', 'DisbursalDate', 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH'], axis=1)\n",
    "\n",
    "## split into train and test sets\n",
    "test_train_emp = emp[emp['Employment.Type'].notnull()]\n",
    "test_y_train_emp = test_train_emp['Employment.Type']\n",
    "test_uniqueid_1 = test_train_emp['UniqueID']\n",
    "test_train_emp = test_train_emp.drop(['Employment.Type', 'UniqueID'], axis=1)\n",
    "\n",
    "test_test_emp = emp[emp['Employment.Type'].isnull()]\n",
    "test_uniqueid_2 = test_test_emp['UniqueID']\n",
    "test_test_emp = test_test_emp.drop(['Employment.Type', 'UniqueID'], axis=1)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "test_train_emp = sc.fit_transform(test_train_emp)\n",
    "test_test_emp = sc.transform(test_test_emp)\n",
    "\n",
    "test_train_emp = pd.DataFrame(test_train_emp)\n",
    "test_test_emp = pd.DataFrame(test_test_emp)\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "test_model_emp = LGBMClassifier()\n",
    "\n",
    "test_model_emp.fit(test_train_emp, test_y_train_emp)\n",
    "\n",
    "test_y_test_emp_pred = test_model_emp.predict(test_test_emp)\n",
    "\n",
    "test_train_emp = pd.DataFrame(test_train_emp)\n",
    "test_test_emp = pd.DataFrame(test_test_emp)\n",
    "test_train_emp['Employment.Type'] = test_y_train_emp.values\n",
    "test_test_emp['Employment.Type'] = test_y_test_emp_pred\n",
    "\n",
    "test_emp_imputed = pd.concat([test_train_emp, test_test_emp], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove attributes : UniqueID, supplier_id, Current_pincode_ID, Employee_code_ID, Date.of.Birth, DisbursalDate\n",
    "\n",
    "y_train = train_emp_imputed['loan_default']\n",
    "train_emp_imputed = train_emp_imputed.drop(['loan_default'], axis=1)\n",
    "unique_id = test_uniqueid_1\n",
    "unique_id = unique_id.append(test_uniqueid_2, ignore_index=True)\n",
    "\n",
    "#train = train.drop(['UniqueID', 'supplier_id', 'Current_pincode_ID', 'Employee_code_ID', 'Date.of.Birth', 'DisbursalDate', 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'Age', 'loan_default'], axis = 1)\n",
    "\n",
    "#test = test.drop(['UniqueID', 'supplier_id', 'Current_pincode_ID', 'Employee_code_ID', 'Date.of.Birth', 'DisbursalDate', 'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'Age'], axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    182543\n",
       "1     50611\n",
       "Name: loan_default, dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Class Imbalance\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "train_emp_imputed[['Employment.Type']] = sc.fit_transform(train_emp_imputed[['Employment.Type']])\n",
    "test_emp_imputed[['Employment.Type']] = sc.transform(test_emp_imputed[['Employment.Type']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (365086, 36)\n",
      "Shape of y: (365086,)\n"
     ]
    }
   ],
   "source": [
    "## Oversampling the minority class (1) SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "x_resample, y_resample = SMOTE().fit_sample(train_emp_imputed, y_train.values.ravel()) \n",
    "\n",
    "# checking the shape of x_resample and y_resample\n",
    "print(\"Shape of x:\", x_resample.shape)\n",
    "print(\"Shape of y:\", y_resample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x161ac2c18>]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=35)\n",
    "\n",
    "pca.fit(x_resample)\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "\n",
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us take 27 components\n",
    "pca = PCA(n_components=27)\n",
    "\n",
    "pca.fit(x_resample)\n",
    "\n",
    "#transform training and test data onto the principal axes\n",
    "pca_x_train = pca.transform(x_resample)\n",
    "pca_x_test = pca.transform(test_emp_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.5329648356825515\n"
     ]
    }
   ],
   "source": [
    "## Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(pca_x_train,y_resample)\n",
    "\n",
    "print(\"Training Accuracy: \", gnb.score(pca_x_train, y_resample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9941137156724716\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(pca_x_train, y_resample)\n",
    "\n",
    "print(\"Training Accuracy: \", model_rf.score(pca_x_train, y_resample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.6398218501942008\n"
     ]
    }
   ],
   "source": [
    "## LightGBM Classifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model_lgb = LGBMClassifier()\n",
    "model_lgb.fit(pca_x_train, y_resample)\n",
    "\n",
    "print(\"Training Accuracy: \", model_lgb.score(pca_x_train, y_resample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictions\n",
    "\n",
    "'''y_pred_gnb = gnb.predict(test_emp_imputed)\n",
    "y_pred_rf = model_rf.predict(test_emp_imputed)\n",
    "y_pred_lgb = model_lgb.predict(test_emp_imputed)\n",
    "'''\n",
    "\n",
    "y_pred_gnb = gnb.predict(pca_x_test)\n",
    "y_pred_rf = model_rf.predict(pca_x_test)\n",
    "y_pred_lgb = model_lgb.predict(pca_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = y_pred_gnb*0.4 + y_pred_lgb*0.3 + y_pred_rf*0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'UniqueID': unique_id,'loan_default': y_pred_rf})\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "##convert numpy arrays to dataframes\n",
    "pca_x_train_df = pd.DataFrame(pca_x_train)\n",
    "pca_x_test_df = pd.DataFrame(pca_x_test)\n",
    "\n",
    "x_train_df = pd.DataFrame(x_resample)\n",
    "x_test_df = pd.DataFrame(test_emp_imputed)\n",
    "\n",
    "y_resample = y_resample.reshape((365086, 1))\n",
    "\n",
    "x_ = x_train_df.values\n",
    "x = pca_x_train_df.values\n",
    "y = y_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((365086, 27), (365086, 1), (365086, 36))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model1\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(64, input_dim=27, activation='relu'))\n",
    "model1.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model1.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_148 (Dense)            (None, 64)                1792      \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 14,337\n",
      "Trainable params: 14,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6595 - acc: 0.6056\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.6504 - acc: 0.6176\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6455 - acc: 0.6235\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6417 - acc: 0.6274\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6381 - acc: 0.6321\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6350 - acc: 0.6348\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6318 - acc: 0.6377\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6288 - acc: 0.6407\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6260 - acc: 0.6433\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6235 - acc: 0.6462\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6210 - acc: 0.6482\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.6185 - acc: 0.6512\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6163 - acc: 0.6533\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.6142 - acc: 0.6551\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6118 - acc: 0.6571\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6100 - acc: 0.6586\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6077 - acc: 0.6611\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6057 - acc: 0.6628\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6040 - acc: 0.6647\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.6019 - acc: 0.6659\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6002 - acc: 0.6679\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5987 - acc: 0.6688\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5973 - acc: 0.6708\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5957 - acc: 0.6719\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5944 - acc: 0.6727\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5925 - acc: 0.6738\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 6s 18us/step - loss: 0.5915 - acc: 0.6750\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5899 - acc: 0.6760\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5888 - acc: 0.6776\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5872 - acc: 0.6786\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5861 - acc: 0.6799\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5850 - acc: 0.6807\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5839 - acc: 0.6811\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5826 - acc: 0.6830\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5817 - acc: 0.6834\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5804 - acc: 0.6847\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5792 - acc: 0.6855\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5786 - acc: 0.6853\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5777 - acc: 0.6864\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5769 - acc: 0.6875\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5761 - acc: 0.6883\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5751 - acc: 0.6884\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5740 - acc: 0.6893\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5733 - acc: 0.6895\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5725 - acc: 0.6903\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5715 - acc: 0.6914\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5707 - acc: 0.6920\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5700 - acc: 0.6925\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5692 - acc: 0.6934\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5681 - acc: 0.6940\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5679 - acc: 0.6943\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5672 - acc: 0.6949\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5663 - acc: 0.6965\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5657 - acc: 0.6958\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5651 - acc: 0.6974\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5645 - acc: 0.6971\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5636 - acc: 0.6976\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5633 - acc: 0.6975\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5627 - acc: 0.6987\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5622 - acc: 0.6993\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5611 - acc: 0.7009\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5602 - acc: 0.7014\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5606 - acc: 0.7013\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5597 - acc: 0.7014\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5588 - acc: 0.7020\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5584 - acc: 0.7024\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5578 - acc: 0.7028\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5576 - acc: 0.7024\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5570 - acc: 0.7036\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5561 - acc: 0.7039\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5560 - acc: 0.7042\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5557 - acc: 0.7042\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5555 - acc: 0.7041\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5547 - acc: 0.7059\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5544 - acc: 0.7059\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5537 - acc: 0.7061\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5534 - acc: 0.7059\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5532 - acc: 0.7068\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5524 - acc: 0.7071\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5525 - acc: 0.7069\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5517 - acc: 0.7077\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5514 - acc: 0.7077\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5512 - acc: 0.7082\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5507 - acc: 0.7086\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5504 - acc: 0.7090\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5500 - acc: 0.7090\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5496 - acc: 0.7090\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5496 - acc: 0.7094\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5493 - acc: 0.7097\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5488 - acc: 0.7101\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5488 - acc: 0.7100\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5482 - acc: 0.7110\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5478 - acc: 0.7109\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5477 - acc: 0.7105\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5479 - acc: 0.7102\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5469 - acc: 0.7112\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5469 - acc: 0.7113\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5465 - acc: 0.7109\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5467 - acc: 0.7118\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5461 - acc: 0.7113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16ad13080>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model2\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_dim=27, activation='tanh'))\n",
    "model2.add(Dense(64, kernel_initializer='normal',activation='tanh'))\n",
    "model2.add(Dense(64, kernel_initializer='normal', activation='tanh'))\n",
    "model2.add(Dense(64, kernel_initializer='normal', activation='tanh'))\n",
    "model2.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6640 - acc: 0.6001\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6588 - acc: 0.6068\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6548 - acc: 0.6134\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6517 - acc: 0.6164\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.6495 - acc: 0.6195\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.6473 - acc: 0.6221\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.6453 - acc: 0.6239\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6436 - acc: 0.6260\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6419 - acc: 0.6272\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6403 - acc: 0.6296\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6387 - acc: 0.6317\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6374 - acc: 0.6323\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.6356 - acc: 0.6347\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 9s 23us/step - loss: 0.6346 - acc: 0.6356\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6331 - acc: 0.6375\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6316 - acc: 0.6392\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6301 - acc: 0.6405\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6288 - acc: 0.6421\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6274 - acc: 0.6437\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6261 - acc: 0.6449\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6246 - acc: 0.6470\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6231 - acc: 0.6477\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6218 - acc: 0.6492\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6203 - acc: 0.6500\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6187 - acc: 0.6525\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6171 - acc: 0.6534\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6154 - acc: 0.6552\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6138 - acc: 0.6569\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6121 - acc: 0.6579\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6104 - acc: 0.6591\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6089 - acc: 0.6611\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6073 - acc: 0.6629\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6054 - acc: 0.6642\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6038 - acc: 0.6658\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.6025 - acc: 0.6668\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6005 - acc: 0.6689\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5990 - acc: 0.6703\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5971 - acc: 0.6724\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5956 - acc: 0.6730\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5937 - acc: 0.6752\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5924 - acc: 0.6777\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5909 - acc: 0.6778\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5893 - acc: 0.6799\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5876 - acc: 0.6813\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5861 - acc: 0.6820\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5849 - acc: 0.6838\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5836 - acc: 0.6840\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5826 - acc: 0.6852\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5808 - acc: 0.6868\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5798 - acc: 0.6879\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5788 - acc: 0.6890\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5775 - acc: 0.6900\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5765 - acc: 0.6907\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5754 - acc: 0.6919\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5744 - acc: 0.6924\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5734 - acc: 0.6931\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5726 - acc: 0.6939\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5715 - acc: 0.6954\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5706 - acc: 0.6961\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5698 - acc: 0.6970\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5691 - acc: 0.6970\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5683 - acc: 0.6975\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5676 - acc: 0.6979\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5667 - acc: 0.6987\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5661 - acc: 0.7003\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5654 - acc: 0.7003\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5648 - acc: 0.7009\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5639 - acc: 0.7013\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5634 - acc: 0.7022\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5627 - acc: 0.7021\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5622 - acc: 0.7025\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5615 - acc: 0.7040\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5610 - acc: 0.7039\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5603 - acc: 0.7042\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5600 - acc: 0.7049\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5593 - acc: 0.7051\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5587 - acc: 0.7067\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5583 - acc: 0.7062\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5578 - acc: 0.7062\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5572 - acc: 0.7073\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5569 - acc: 0.7076\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5565 - acc: 0.7080\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5557 - acc: 0.7084\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5553 - acc: 0.7092\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5551 - acc: 0.7090\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5546 - acc: 0.7098\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5542 - acc: 0.7096\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5536 - acc: 0.7109\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5534 - acc: 0.7103\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5529 - acc: 0.7110\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5527 - acc: 0.7109\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5526 - acc: 0.7111\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5518 - acc: 0.7112\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5515 - acc: 0.7116\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5510 - acc: 0.7126\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5509 - acc: 0.7123\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5505 - acc: 0.7124\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5505 - acc: 0.7125\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5498 - acc: 0.7134\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 8s 23us/step - loss: 0.5494 - acc: 0.7133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122d5d9b0>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model3\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(64, input_dim=27, activation='relu'))\n",
    "model3.add(Dense(64, kernel_initializer='normal',activation='relu'))\n",
    "model3.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model3.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6600 - acc: 0.6059\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6510 - acc: 0.6175\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6461 - acc: 0.6227\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.6421 - acc: 0.6280\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6381 - acc: 0.6323\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6348 - acc: 0.6353\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6318 - acc: 0.6385\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6291 - acc: 0.6419\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6265 - acc: 0.6443\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6238 - acc: 0.6466\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6214 - acc: 0.6495\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6192 - acc: 0.6505\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6166 - acc: 0.6542\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6145 - acc: 0.6552\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6126 - acc: 0.6573\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6106 - acc: 0.6588\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6089 - acc: 0.6600\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6068 - acc: 0.6621\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6053 - acc: 0.6634\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6032 - acc: 0.6652\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6019 - acc: 0.6667\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.6001 - acc: 0.6683\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5987 - acc: 0.6688\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5974 - acc: 0.6700\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5959 - acc: 0.6711\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5945 - acc: 0.6724\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5930 - acc: 0.6744\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5920 - acc: 0.6745\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5906 - acc: 0.6754\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5892 - acc: 0.6768\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5879 - acc: 0.6774\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5867 - acc: 0.6789\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5854 - acc: 0.6799\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5846 - acc: 0.6802\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5835 - acc: 0.6812\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5823 - acc: 0.6824\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5814 - acc: 0.6831\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5804 - acc: 0.6838\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5794 - acc: 0.6847\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5781 - acc: 0.6858\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5776 - acc: 0.6866\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5767 - acc: 0.6875\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5759 - acc: 0.6888\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5745 - acc: 0.6887\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5743 - acc: 0.6891\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5735 - acc: 0.6898\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5726 - acc: 0.6903\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5717 - acc: 0.6908\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5708 - acc: 0.6917\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5704 - acc: 0.6921\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5698 - acc: 0.6924\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5691 - acc: 0.6934\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5682 - acc: 0.6943\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5677 - acc: 0.6951\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5669 - acc: 0.6955\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5667 - acc: 0.6946\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5659 - acc: 0.6954\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5656 - acc: 0.6956\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5646 - acc: 0.6973\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5639 - acc: 0.6981\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5635 - acc: 0.6978\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5632 - acc: 0.6987\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5628 - acc: 0.6988\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5621 - acc: 0.6989\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5612 - acc: 0.6997\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5611 - acc: 0.6992\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5607 - acc: 0.6999\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5601 - acc: 0.7006\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5595 - acc: 0.7013\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5591 - acc: 0.7014\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5586 - acc: 0.7025\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5579 - acc: 0.7026\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5579 - acc: 0.7028\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5573 - acc: 0.7030\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5568 - acc: 0.7034\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5564 - acc: 0.7037\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5559 - acc: 0.7041\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5555 - acc: 0.7045\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5553 - acc: 0.7053\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5544 - acc: 0.7053\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5543 - acc: 0.7052\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5537 - acc: 0.7065\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5538 - acc: 0.7056\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5533 - acc: 0.7063\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5526 - acc: 0.7072\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5524 - acc: 0.7074\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5518 - acc: 0.7080\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5515 - acc: 0.7068\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5516 - acc: 0.7081\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5511 - acc: 0.7085\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5507 - acc: 0.7079\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5504 - acc: 0.7088\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5500 - acc: 0.7096\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5496 - acc: 0.7086\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5495 - acc: 0.7092\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5492 - acc: 0.7095\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5487 - acc: 0.7099\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5490 - acc: 0.7099\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5482 - acc: 0.7101\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5478 - acc: 0.7111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12709abe0>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model4\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(64, input_dim=27, activation='tanh'))\n",
    "model4.add(Dense(64, kernel_initializer='normal', activation='tanh'))\n",
    "model4.add(Dense(64, kernel_initializer='normal', activation='tanh'))\n",
    "model4.add(Dense(64, kernel_initializer='normal',activation='tanh'))\n",
    "model4.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6641 - acc: 0.5996\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6583 - acc: 0.6077\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6542 - acc: 0.6130\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6518 - acc: 0.6157\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6495 - acc: 0.6190\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6473 - acc: 0.6215\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.6456 - acc: 0.6236\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6441 - acc: 0.6247\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6425 - acc: 0.6266\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6410 - acc: 0.6282\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 7s 21us/step - loss: 0.6394 - acc: 0.6302\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.6380 - acc: 0.6321\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 7s 20us/step - loss: 0.6365 - acc: 0.6333\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 7s 20us/step - loss: 0.6351 - acc: 0.6341\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6336 - acc: 0.6357\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.6322 - acc: 0.6375\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6309 - acc: 0.6390\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 8s 21us/step - loss: 0.6294 - acc: 0.6405\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6281 - acc: 0.6414\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6268 - acc: 0.6428\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 6s 18us/step - loss: 0.6251 - acc: 0.6442\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6239 - acc: 0.6450\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6224 - acc: 0.6468\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6208 - acc: 0.6480\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6193 - acc: 0.6494\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.6179 - acc: 0.6507\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6162 - acc: 0.6526\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6148 - acc: 0.6535\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6131 - acc: 0.6561\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6117 - acc: 0.6568\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6100 - acc: 0.6579\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6084 - acc: 0.6599\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.6068 - acc: 0.6616\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6051 - acc: 0.6627\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6033 - acc: 0.6647\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6015 - acc: 0.6670\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5999 - acc: 0.6681\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5983 - acc: 0.6704\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.5965 - acc: 0.6716\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 7s 20us/step - loss: 0.5948 - acc: 0.6728\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5930 - acc: 0.6747\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5915 - acc: 0.6763\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5897 - acc: 0.6775\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5883 - acc: 0.6788\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5868 - acc: 0.6801\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5857 - acc: 0.6814\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5841 - acc: 0.6824\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5828 - acc: 0.6835\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5812 - acc: 0.6857\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5803 - acc: 0.6861\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5791 - acc: 0.6866\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5777 - acc: 0.6881\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5769 - acc: 0.6892\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5755 - acc: 0.6904\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5746 - acc: 0.6914\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5734 - acc: 0.6929\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5727 - acc: 0.6937\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 6s 18us/step - loss: 0.5715 - acc: 0.6943\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 10s 26us/step - loss: 0.5706 - acc: 0.6946\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.5696 - acc: 0.6962\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5688 - acc: 0.6969\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.5679 - acc: 0.6979\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5672 - acc: 0.6983\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5660 - acc: 0.6990\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5653 - acc: 0.6998\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5646 - acc: 0.7013\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5636 - acc: 0.7017\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5628 - acc: 0.7028\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5623 - acc: 0.7025\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5617 - acc: 0.7032\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.5608 - acc: 0.7044\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5604 - acc: 0.7043\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5597 - acc: 0.7049\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5593 - acc: 0.7054\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5588 - acc: 0.7060\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5584 - acc: 0.7064\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5577 - acc: 0.7072\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5572 - acc: 0.7073\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5571 - acc: 0.7077\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5563 - acc: 0.7087\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 10s 27us/step - loss: 0.5560 - acc: 0.7089\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 10s 28us/step - loss: 0.5556 - acc: 0.7090\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 10s 28us/step - loss: 0.5549 - acc: 0.7090\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 10s 28us/step - loss: 0.5544 - acc: 0.7100\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 10s 28us/step - loss: 0.5544 - acc: 0.7095\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 10s 28us/step - loss: 0.5541 - acc: 0.7105\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 10s 28us/step - loss: 0.5537 - acc: 0.7101\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 10s 28us/step - loss: 0.5531 - acc: 0.7106\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 10s 28us/step - loss: 0.5529 - acc: 0.7104\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 10s 26us/step - loss: 0.5528 - acc: 0.7114\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5520 - acc: 0.7118\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5519 - acc: 0.7119\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5517 - acc: 0.7121\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5513 - acc: 0.7127\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5511 - acc: 0.7130\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5507 - acc: 0.7126\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5500 - acc: 0.7130\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5500 - acc: 0.7141\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.5498 - acc: 0.7141\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 9s 25us/step - loss: 0.5494 - acc: 0.7147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12cbd5e10>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model5\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(64, input_dim=27, activation='relu'))\n",
    "model5.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model5.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6595 - acc: 0.6056\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6505 - acc: 0.6180\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6455 - acc: 0.6236\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.6417 - acc: 0.6282\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6380 - acc: 0.6322\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6342 - acc: 0.6367\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6310 - acc: 0.6401\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.6278 - acc: 0.6430\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 7s 20us/step - loss: 0.6251 - acc: 0.6455\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6221 - acc: 0.6490\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6197 - acc: 0.6519\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6170 - acc: 0.6539\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.6153 - acc: 0.6556\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6127 - acc: 0.6572\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6107 - acc: 0.6597\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6090 - acc: 0.6621\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6069 - acc: 0.6631\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6051 - acc: 0.6651\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6029 - acc: 0.6670\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6013 - acc: 0.6683\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5996 - acc: 0.6700\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5981 - acc: 0.6713\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5967 - acc: 0.6730\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5951 - acc: 0.6735\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5939 - acc: 0.6743\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5923 - acc: 0.6766\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 6s 18us/step - loss: 0.5909 - acc: 0.6772\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 7s 20us/step - loss: 0.5897 - acc: 0.6782\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5886 - acc: 0.6795\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5872 - acc: 0.6804\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5863 - acc: 0.6814\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5850 - acc: 0.6822\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5840 - acc: 0.6835\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5832 - acc: 0.6837\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5816 - acc: 0.6851\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5807 - acc: 0.6862\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5797 - acc: 0.6869\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5792 - acc: 0.6876\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5783 - acc: 0.6883\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5767 - acc: 0.6894\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5762 - acc: 0.6898\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5753 - acc: 0.6911\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5738 - acc: 0.6921\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5734 - acc: 0.6920\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5725 - acc: 0.6936\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5715 - acc: 0.6939\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5708 - acc: 0.6944\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5698 - acc: 0.6951\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5696 - acc: 0.6953\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5690 - acc: 0.6958\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5681 - acc: 0.6980\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5674 - acc: 0.6980\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5667 - acc: 0.6985\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5661 - acc: 0.6986\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5653 - acc: 0.6991\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5647 - acc: 0.6998\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5646 - acc: 0.6996\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5634 - acc: 0.7012\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5630 - acc: 0.7017\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5625 - acc: 0.7016\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5618 - acc: 0.7020\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5613 - acc: 0.7020\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5606 - acc: 0.7030\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5599 - acc: 0.7034\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5595 - acc: 0.7035\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5594 - acc: 0.7037\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.5586 - acc: 0.7038\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5581 - acc: 0.7042\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5575 - acc: 0.7050\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5566 - acc: 0.7058\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5561 - acc: 0.7068\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5565 - acc: 0.7061\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5557 - acc: 0.7062\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5550 - acc: 0.7071\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5547 - acc: 0.7072\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5539 - acc: 0.7082\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5541 - acc: 0.7079\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5539 - acc: 0.7084\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5532 - acc: 0.7090\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5527 - acc: 0.7088\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5523 - acc: 0.7095\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5519 - acc: 0.7098\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5514 - acc: 0.7102\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5511 - acc: 0.7108\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5510 - acc: 0.7105\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5505 - acc: 0.7107\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5504 - acc: 0.7108\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5496 - acc: 0.7114\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5496 - acc: 0.7119\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5491 - acc: 0.7119\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5485 - acc: 0.7128\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5483 - acc: 0.7121\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5482 - acc: 0.7127\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5478 - acc: 0.7134\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5473 - acc: 0.7138\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5473 - acc: 0.7141\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5467 - acc: 0.7138\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5467 - acc: 0.7143\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5460 - acc: 0.7146\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5457 - acc: 0.7148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15b2f70f0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model6\n",
    "model6 = Sequential()\n",
    "model6.add(Dense(64, input_dim=27, activation='relu'))\n",
    "model6.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model6.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model6.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model6.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model6.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model6.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6597 - acc: 0.6063\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6509 - acc: 0.6184\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6460 - acc: 0.6225\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6422 - acc: 0.6267\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6385 - acc: 0.6308\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6349 - acc: 0.6351\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6317 - acc: 0.6388\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6287 - acc: 0.6413\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6258 - acc: 0.6445\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6230 - acc: 0.6477\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6204 - acc: 0.6490\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6181 - acc: 0.6521\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6159 - acc: 0.6537\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6136 - acc: 0.6558\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6112 - acc: 0.6585\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6096 - acc: 0.6588\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6070 - acc: 0.6617\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6055 - acc: 0.6627\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6034 - acc: 0.6645\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6014 - acc: 0.6672\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5996 - acc: 0.6681\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5981 - acc: 0.6695\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5964 - acc: 0.6714\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5949 - acc: 0.6722\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5933 - acc: 0.6739\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5918 - acc: 0.6758\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5903 - acc: 0.6762\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5889 - acc: 0.6786\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 8s 21us/step - loss: 0.5876 - acc: 0.6793\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5863 - acc: 0.6792\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5844 - acc: 0.6817\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5837 - acc: 0.6820\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5821 - acc: 0.6828\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5809 - acc: 0.6849\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.5799 - acc: 0.6856\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5785 - acc: 0.6861\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5777 - acc: 0.6869\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5762 - acc: 0.6884\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5749 - acc: 0.6893\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5736 - acc: 0.6908\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.5729 - acc: 0.6912\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5719 - acc: 0.6918\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5705 - acc: 0.6932\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5692 - acc: 0.6938\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5687 - acc: 0.6941\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5672 - acc: 0.6955\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5664 - acc: 0.6957\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5659 - acc: 0.6963\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5648 - acc: 0.6975\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5640 - acc: 0.6974\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 6s 18us/step - loss: 0.5631 - acc: 0.6992\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5624 - acc: 0.6993\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5617 - acc: 0.7000\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5606 - acc: 0.7011\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5598 - acc: 0.7006\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5588 - acc: 0.7022\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5584 - acc: 0.7024\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5576 - acc: 0.7030\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5569 - acc: 0.7031\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5562 - acc: 0.7040\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5555 - acc: 0.7043\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5546 - acc: 0.7056\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5544 - acc: 0.7054\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5535 - acc: 0.7067\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5529 - acc: 0.7074\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5522 - acc: 0.7079\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5515 - acc: 0.7086\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5514 - acc: 0.7078\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5509 - acc: 0.7086\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5500 - acc: 0.7095\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5494 - acc: 0.7096\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5487 - acc: 0.7109\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5487 - acc: 0.7108\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5483 - acc: 0.7107\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5474 - acc: 0.7113\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5474 - acc: 0.7109\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5465 - acc: 0.7117\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5455 - acc: 0.7129\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5454 - acc: 0.7129\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5453 - acc: 0.7132\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5441 - acc: 0.7133\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5440 - acc: 0.7135\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5435 - acc: 0.7140\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5429 - acc: 0.7145\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5422 - acc: 0.7147\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5423 - acc: 0.7151\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5425 - acc: 0.7142\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5412 - acc: 0.7155\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5413 - acc: 0.7160\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5408 - acc: 0.7161\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5406 - acc: 0.7157\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5395 - acc: 0.7168\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5392 - acc: 0.7177\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5391 - acc: 0.7171\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5390 - acc: 0.7171\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5384 - acc: 0.7181\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5382 - acc: 0.7178\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5376 - acc: 0.7177\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5369 - acc: 0.7188\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5369 - acc: 0.7187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15b777080>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model7\n",
    "model7 = Sequential()\n",
    "model7.add(Dense(64, input_dim=27, activation='relu'))\n",
    "model7.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model7.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model7.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6602 - acc: 0.6053\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.6511 - acc: 0.6167\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6458 - acc: 0.6223\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6410 - acc: 0.6279\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6368 - acc: 0.6338\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6330 - acc: 0.6374\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6296 - acc: 0.6410\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6263 - acc: 0.6446\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6233 - acc: 0.6479\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6201 - acc: 0.6516\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6176 - acc: 0.6532\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6151 - acc: 0.6566\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6126 - acc: 0.6580\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6104 - acc: 0.6604\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6081 - acc: 0.6620\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 8s 22us/step - loss: 0.6061 - acc: 0.6639\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.6043 - acc: 0.6664\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6023 - acc: 0.6681\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6003 - acc: 0.6699\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5983 - acc: 0.6724\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5970 - acc: 0.6728\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5949 - acc: 0.6741\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5931 - acc: 0.6762\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5919 - acc: 0.6774\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5901 - acc: 0.6786\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5884 - acc: 0.6805\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5870 - acc: 0.6809\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5857 - acc: 0.6825\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5841 - acc: 0.6833\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5826 - acc: 0.6850\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5816 - acc: 0.6854\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5801 - acc: 0.6864\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5790 - acc: 0.6880\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5775 - acc: 0.6887\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5763 - acc: 0.6901\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5753 - acc: 0.6909\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5743 - acc: 0.6911\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5734 - acc: 0.6918\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5722 - acc: 0.6933\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5712 - acc: 0.6936\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5698 - acc: 0.6956\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5694 - acc: 0.6948\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.5679 - acc: 0.6962\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5671 - acc: 0.6968\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5663 - acc: 0.6973\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5651 - acc: 0.6986\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5643 - acc: 0.6993\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5637 - acc: 0.6993\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5624 - acc: 0.7007\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5619 - acc: 0.7010\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5608 - acc: 0.7020\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5602 - acc: 0.7024\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5593 - acc: 0.7026\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5587 - acc: 0.7027\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5577 - acc: 0.7042\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5573 - acc: 0.7038\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5563 - acc: 0.7047\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5554 - acc: 0.7055\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5550 - acc: 0.7057\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5538 - acc: 0.7072\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5535 - acc: 0.7075\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5532 - acc: 0.7067\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5525 - acc: 0.7079\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5517 - acc: 0.7086\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5514 - acc: 0.7083\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5504 - acc: 0.7098\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5500 - acc: 0.7100\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5496 - acc: 0.7100\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5484 - acc: 0.7106\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5483 - acc: 0.7112\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5473 - acc: 0.7115\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5470 - acc: 0.7120\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5468 - acc: 0.7117\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5459 - acc: 0.7130\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5454 - acc: 0.7131\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5450 - acc: 0.7136\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5445 - acc: 0.7138\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5432 - acc: 0.7147\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5430 - acc: 0.7147\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5431 - acc: 0.7146\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5421 - acc: 0.7160\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5423 - acc: 0.7155\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5411 - acc: 0.7159\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5413 - acc: 0.7159\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5403 - acc: 0.7173\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5399 - acc: 0.7173\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5392 - acc: 0.7180\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5391 - acc: 0.7178\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5392 - acc: 0.7181\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5378 - acc: 0.7184\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5382 - acc: 0.7181\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5378 - acc: 0.7184\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5376 - acc: 0.7191\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5372 - acc: 0.7187\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5362 - acc: 0.7201\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5363 - acc: 0.7193\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5358 - acc: 0.7193\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5352 - acc: 0.7206\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 6s 18us/step - loss: 0.5345 - acc: 0.7209\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 9s 26us/step - loss: 0.5345 - acc: 0.7206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c7a7898>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model8\n",
    "model8 = Sequential()\n",
    "model8.add(Dense(64, input_dim=27, activation='relu'))\n",
    "model8.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model8.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model8.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model8.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model8.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6595 - acc: 0.6058\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6501 - acc: 0.6190\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6452 - acc: 0.6242\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6411 - acc: 0.6291\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6378 - acc: 0.6324\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6343 - acc: 0.6365\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6313 - acc: 0.6397\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6285 - acc: 0.6422\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6257 - acc: 0.6451\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6233 - acc: 0.6467\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.6207 - acc: 0.6495\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6185 - acc: 0.6511\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.6164 - acc: 0.6531\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6142 - acc: 0.6555\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6121 - acc: 0.6579\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6105 - acc: 0.6589\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6086 - acc: 0.6606\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6066 - acc: 0.6634\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6051 - acc: 0.6642\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6036 - acc: 0.6655\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6019 - acc: 0.6671\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.6004 - acc: 0.6677\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5990 - acc: 0.6695\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5973 - acc: 0.6707\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5960 - acc: 0.6721\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5950 - acc: 0.6728\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5934 - acc: 0.6736\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5921 - acc: 0.6746\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5913 - acc: 0.6756\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5897 - acc: 0.6773\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5885 - acc: 0.6785\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5873 - acc: 0.6790\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5861 - acc: 0.6807\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5851 - acc: 0.6808\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5841 - acc: 0.6812\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5831 - acc: 0.6830\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5819 - acc: 0.6832\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5813 - acc: 0.6841\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5798 - acc: 0.6859\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5789 - acc: 0.6862\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5779 - acc: 0.6868\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5775 - acc: 0.6878\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5763 - acc: 0.6892\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5757 - acc: 0.6891\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5746 - acc: 0.6904\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5737 - acc: 0.6914\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5731 - acc: 0.6918\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5725 - acc: 0.6924\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5715 - acc: 0.6935\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5713 - acc: 0.6937\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5707 - acc: 0.6938\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5695 - acc: 0.6949\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5690 - acc: 0.6953\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5684 - acc: 0.6964\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5678 - acc: 0.6961\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5669 - acc: 0.6967\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5666 - acc: 0.6966\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5659 - acc: 0.6984\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5652 - acc: 0.6992\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5647 - acc: 0.6989\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5643 - acc: 0.6999\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5639 - acc: 0.6999\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5630 - acc: 0.7009\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5627 - acc: 0.7007\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5627 - acc: 0.7016\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5616 - acc: 0.7020\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5609 - acc: 0.7024\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5609 - acc: 0.7026\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5601 - acc: 0.7031\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5603 - acc: 0.7029\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5590 - acc: 0.7039\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5589 - acc: 0.7042\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5584 - acc: 0.7049\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5574 - acc: 0.7053\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5574 - acc: 0.7058\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5572 - acc: 0.7060\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5566 - acc: 0.7063\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5561 - acc: 0.7066\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5563 - acc: 0.7058\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5557 - acc: 0.7077\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5548 - acc: 0.7075\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5550 - acc: 0.7075\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5541 - acc: 0.7080\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5537 - acc: 0.7078\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5537 - acc: 0.7085\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5532 - acc: 0.7087\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5535 - acc: 0.7092\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5527 - acc: 0.7095\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5523 - acc: 0.7096\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5523 - acc: 0.7094\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5515 - acc: 0.7099\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5516 - acc: 0.7089\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 7s 18us/step - loss: 0.5510 - acc: 0.7102\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5506 - acc: 0.7104\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5505 - acc: 0.7108\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5502 - acc: 0.7110\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5500 - acc: 0.7112\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5499 - acc: 0.7114\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5496 - acc: 0.7119\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5494 - acc: 0.7119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x134d07320>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model9\n",
    "model9 = Sequential()\n",
    "model9.add(Dense(64, input_dim=27, activation='relu'))\n",
    "model9.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model9.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model9.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model9.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model9.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.6600 - acc: 0.6056\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.6513 - acc: 0.6167\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6469 - acc: 0.6215\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6429 - acc: 0.6260\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6395 - acc: 0.6289\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6363 - acc: 0.6318\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6334 - acc: 0.6355\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6304 - acc: 0.6390\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6276 - acc: 0.6426\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6249 - acc: 0.6444\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6220 - acc: 0.6482\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6193 - acc: 0.6504\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6166 - acc: 0.6537\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6145 - acc: 0.6555\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.6123 - acc: 0.6575\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6098 - acc: 0.6604\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 9s 24us/step - loss: 0.6076 - acc: 0.6625\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6058 - acc: 0.6638\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 8s 21us/step - loss: 0.6039 - acc: 0.6650\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6021 - acc: 0.6676\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6006 - acc: 0.6693\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 9s 23us/step - loss: 0.5991 - acc: 0.6693\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 7s 19us/step - loss: 0.5975 - acc: 0.6713\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5959 - acc: 0.6729\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5941 - acc: 0.6747\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5932 - acc: 0.6749\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5917 - acc: 0.6761\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5908 - acc: 0.6775\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5896 - acc: 0.6784\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5883 - acc: 0.6791\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5875 - acc: 0.6798\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5859 - acc: 0.6812\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5852 - acc: 0.6810\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5840 - acc: 0.6826\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5832 - acc: 0.6837\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5820 - acc: 0.6847\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5811 - acc: 0.6852\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5803 - acc: 0.6863\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5788 - acc: 0.6880\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5783 - acc: 0.6873\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5776 - acc: 0.6884\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5768 - acc: 0.6888\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5757 - acc: 0.6895\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5752 - acc: 0.6905\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5739 - acc: 0.6909\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5732 - acc: 0.6923\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5722 - acc: 0.6931\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.5724 - acc: 0.6925\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5711 - acc: 0.6939\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5708 - acc: 0.6936\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5697 - acc: 0.6948\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5690 - acc: 0.6959\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5680 - acc: 0.6957\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5677 - acc: 0.6960\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5669 - acc: 0.6970\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5663 - acc: 0.6971\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5655 - acc: 0.6977\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 4s 12us/step - loss: 0.5651 - acc: 0.6981\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5643 - acc: 0.6988\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5641 - acc: 0.6988\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5632 - acc: 0.6994\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5628 - acc: 0.7005\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5621 - acc: 0.7002\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5618 - acc: 0.7008\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5611 - acc: 0.7010\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5607 - acc: 0.7014\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5601 - acc: 0.7018\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5600 - acc: 0.7026\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 4s 11us/step - loss: 0.5591 - acc: 0.7027\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 4s 10us/step - loss: 0.5588 - acc: 0.7031\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.5583 - acc: 0.7031\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5579 - acc: 0.7031\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5577 - acc: 0.7031\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5572 - acc: 0.7043\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5568 - acc: 0.7050\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5566 - acc: 0.7048\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5555 - acc: 0.7057\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.5551 - acc: 0.7057\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5554 - acc: 0.7052\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5546 - acc: 0.7058\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5541 - acc: 0.7065\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5538 - acc: 0.7071\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5533 - acc: 0.7075\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5533 - acc: 0.7059\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5524 - acc: 0.7079\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5527 - acc: 0.7075\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5521 - acc: 0.7078\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5519 - acc: 0.7083\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5515 - acc: 0.7087\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5511 - acc: 0.7082\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5510 - acc: 0.7084\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5504 - acc: 0.7094\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5503 - acc: 0.7094\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5499 - acc: 0.7101\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5499 - acc: 0.7098\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5493 - acc: 0.7098\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5494 - acc: 0.7103\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5489 - acc: 0.7108\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5485 - acc: 0.7106\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5482 - acc: 0.7109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136b80c50>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model10\n",
    "model10 = Sequential()\n",
    "model10.add(Dense(64, input_dim=27, activation='relu'))\n",
    "model10.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model10.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model10.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "model10.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# Compile model1\n",
    "model10.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "365086/365086 [==============================] - 5s 12us/step - loss: 0.6604 - acc: 0.6033\n",
      "Epoch 2/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6510 - acc: 0.6173\n",
      "Epoch 3/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6462 - acc: 0.6222\n",
      "Epoch 4/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6420 - acc: 0.6273\n",
      "Epoch 5/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6386 - acc: 0.6310\n",
      "Epoch 6/100\n",
      "365086/365086 [==============================] - 5s 14us/step - loss: 0.6352 - acc: 0.6349\n",
      "Epoch 7/100\n",
      "365086/365086 [==============================] - 5s 13us/step - loss: 0.6324 - acc: 0.6377\n",
      "Epoch 8/100\n",
      "365086/365086 [==============================] - 5s 15us/step - loss: 0.6295 - acc: 0.6412\n",
      "Epoch 9/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6268 - acc: 0.6436\n",
      "Epoch 10/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6242 - acc: 0.6468\n",
      "Epoch 11/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6218 - acc: 0.6491\n",
      "Epoch 12/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6197 - acc: 0.6514\n",
      "Epoch 13/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6173 - acc: 0.6540\n",
      "Epoch 14/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6149 - acc: 0.6568\n",
      "Epoch 15/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6130 - acc: 0.6577\n",
      "Epoch 16/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6112 - acc: 0.6598\n",
      "Epoch 17/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6092 - acc: 0.6619\n",
      "Epoch 18/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6074 - acc: 0.6638\n",
      "Epoch 19/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6059 - acc: 0.6646\n",
      "Epoch 20/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6038 - acc: 0.6661\n",
      "Epoch 21/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.6023 - acc: 0.6675\n",
      "Epoch 22/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.6004 - acc: 0.6700\n",
      "Epoch 23/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5990 - acc: 0.6699\n",
      "Epoch 24/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5976 - acc: 0.6719\n",
      "Epoch 25/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5963 - acc: 0.6729\n",
      "Epoch 26/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5947 - acc: 0.6750\n",
      "Epoch 27/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5934 - acc: 0.6757\n",
      "Epoch 28/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5924 - acc: 0.6766\n",
      "Epoch 29/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5909 - acc: 0.6776\n",
      "Epoch 30/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5896 - acc: 0.6795\n",
      "Epoch 31/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5883 - acc: 0.6805\n",
      "Epoch 32/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5871 - acc: 0.6812\n",
      "Epoch 33/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5865 - acc: 0.6820\n",
      "Epoch 34/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5852 - acc: 0.6821\n",
      "Epoch 35/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5839 - acc: 0.6842\n",
      "Epoch 36/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5829 - acc: 0.6841\n",
      "Epoch 37/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5824 - acc: 0.6847\n",
      "Epoch 38/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5810 - acc: 0.6862\n",
      "Epoch 39/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5802 - acc: 0.6867\n",
      "Epoch 40/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5789 - acc: 0.6876\n",
      "Epoch 41/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5784 - acc: 0.6884\n",
      "Epoch 42/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5779 - acc: 0.6885\n",
      "Epoch 43/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5768 - acc: 0.6891\n",
      "Epoch 44/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5756 - acc: 0.6900\n",
      "Epoch 45/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5751 - acc: 0.6908\n",
      "Epoch 46/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5743 - acc: 0.6916\n",
      "Epoch 47/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5736 - acc: 0.6923\n",
      "Epoch 48/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5727 - acc: 0.6926\n",
      "Epoch 49/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5722 - acc: 0.6934\n",
      "Epoch 50/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5710 - acc: 0.6949\n",
      "Epoch 51/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5708 - acc: 0.6944\n",
      "Epoch 52/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5701 - acc: 0.6947\n",
      "Epoch 53/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5695 - acc: 0.6957\n",
      "Epoch 54/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5687 - acc: 0.6957\n",
      "Epoch 55/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5685 - acc: 0.6961\n",
      "Epoch 56/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5677 - acc: 0.6973\n",
      "Epoch 57/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5671 - acc: 0.6971\n",
      "Epoch 58/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5666 - acc: 0.6969\n",
      "Epoch 59/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5659 - acc: 0.6983\n",
      "Epoch 60/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5654 - acc: 0.6977\n",
      "Epoch 61/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5651 - acc: 0.6990\n",
      "Epoch 62/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5647 - acc: 0.6989\n",
      "Epoch 63/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5641 - acc: 0.6993\n",
      "Epoch 64/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5634 - acc: 0.7000\n",
      "Epoch 65/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5629 - acc: 0.7004\n",
      "Epoch 66/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5627 - acc: 0.7007\n",
      "Epoch 67/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5621 - acc: 0.7013\n",
      "Epoch 68/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5616 - acc: 0.7015\n",
      "Epoch 69/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5611 - acc: 0.7012\n",
      "Epoch 70/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5607 - acc: 0.7023\n",
      "Epoch 71/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5601 - acc: 0.7017\n",
      "Epoch 72/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5602 - acc: 0.7023\n",
      "Epoch 73/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5591 - acc: 0.7032\n",
      "Epoch 74/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5592 - acc: 0.7033\n",
      "Epoch 75/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5589 - acc: 0.7032\n",
      "Epoch 76/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5582 - acc: 0.7038\n",
      "Epoch 77/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5574 - acc: 0.7050\n",
      "Epoch 78/100\n",
      "365086/365086 [==============================] - 6s 15us/step - loss: 0.5574 - acc: 0.7039\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5567 - acc: 0.7052\n",
      "Epoch 80/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5564 - acc: 0.7052\n",
      "Epoch 81/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5565 - acc: 0.7054\n",
      "Epoch 82/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5556 - acc: 0.7064\n",
      "Epoch 83/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5551 - acc: 0.7067\n",
      "Epoch 84/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5549 - acc: 0.7068\n",
      "Epoch 85/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5549 - acc: 0.7066\n",
      "Epoch 86/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5544 - acc: 0.7074\n",
      "Epoch 87/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5540 - acc: 0.7071\n",
      "Epoch 88/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5536 - acc: 0.7073\n",
      "Epoch 89/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5533 - acc: 0.7077\n",
      "Epoch 90/100\n",
      "365086/365086 [==============================] - 6s 17us/step - loss: 0.5527 - acc: 0.7076\n",
      "Epoch 91/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5523 - acc: 0.7081\n",
      "Epoch 92/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5521 - acc: 0.7081\n",
      "Epoch 93/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5519 - acc: 0.7089\n",
      "Epoch 94/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5518 - acc: 0.7092\n",
      "Epoch 95/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5515 - acc: 0.7088\n",
      "Epoch 96/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5510 - acc: 0.7099\n",
      "Epoch 97/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5511 - acc: 0.7095\n",
      "Epoch 98/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5504 - acc: 0.7097\n",
      "Epoch 99/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5503 - acc: 0.7098\n",
      "Epoch 100/100\n",
      "365086/365086 [==============================] - 6s 16us/step - loss: 0.5496 - acc: 0.7100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x136b807f0>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model10.fit(x, y, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_df = pd.DataFrame(test_emp_imputed)\n",
    "x_test = pca_x_test_df.values\n",
    "x_t = x_test_df.values\n",
    "\n",
    "predictions1 = model1.predict(x_test)\n",
    "predictions2 = model2.predict(x_test)\n",
    "predictions3 = model3.predict(x_test)\n",
    "\n",
    "#predictions = (predictions1 + predictions2 + predictions3 + y_pred_gnb.reshape((112392, 1)) + y_pred_lgb.reshape((112392, 1)) + y_pred_rf.reshape((112392, 1)))/6.0\n",
    "\n",
    "predictions4 = model4.predict(x_test)\n",
    "predictions5 = model5.predict(x_test)\n",
    "predictions6 = model6.predict(x_test)\n",
    "predictions7 = model7.predict(x_test)\n",
    "predictions8 = model8.predict(x_test)\n",
    "predictions9 = model9.predict(x_test)\n",
    "predictions10 = model10.predict(x_test)\n",
    "\n",
    "predictions = (predictions1 + predictions2 + predictions3 + predictions4 + predictions5 + predictions6 + predictions7 + predictions8 + predictions9 + predictions10)/10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112392,)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = map(lambda x: x[0], predictions)\n",
    "pred = pd.Series(my_list)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = (y_pred_gnb + y_pred_lgb + y_pred_rf)/3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = 0.6*pred + 0.4*pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'UniqueID': unique_id,'loan_default': final_pred})\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "lgb_params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"auc\",\n",
    "    \"boosting\": 'gbdt',\n",
    "    \"max_depth\" : -1,\n",
    "    \"num_leaves\" : 13,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\" : 0.4,\n",
    "    \"feature_fraction\" : 0.05,\n",
    "    \"min_data_in_leaf\": 80,\n",
    "    \"min_sum_heassian_in_leaf\": 10,\n",
    "    \"tree_learner\": \"serial\",\n",
    "    \"boost_from_average\": \"false\",\n",
    "    #\"lambda_l1\" : 5,\n",
    "    #\"lambda_l2\" : 5,\n",
    "    \"bagging_seed\" : random_state,\n",
    "    \"verbosity\" : 1,\n",
    "    \"seed\": random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "predictions = unique_id\n",
    "val_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train = y_train.ravel()\n",
    "y_val = y_val.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n",
      "Training until validation scores don't improve for 3000 rounds.\n",
      "[1000]\ttraining's auc: 0.646026\tvalid_1's auc: 0.640542\n",
      "[2000]\ttraining's auc: 0.655205\tvalid_1's auc: 0.647179\n",
      "[3000]\ttraining's auc: 0.660966\tvalid_1's auc: 0.650658\n",
      "[4000]\ttraining's auc: 0.664992\tvalid_1's auc: 0.652685\n",
      "[5000]\ttraining's auc: 0.668052\tvalid_1's auc: 0.653825\n",
      "[6000]\ttraining's auc: 0.670599\tvalid_1's auc: 0.654484\n",
      "[7000]\ttraining's auc: 0.672538\tvalid_1's auc: 0.654969\n",
      "[8000]\ttraining's auc: 0.674135\tvalid_1's auc: 0.655186\n",
      "[9000]\ttraining's auc: 0.675477\tvalid_1's auc: 0.655184\n",
      "[10000]\ttraining's auc: 0.676765\tvalid_1's auc: 0.655116\n",
      "[11000]\ttraining's auc: 0.677766\tvalid_1's auc: 0.654896\n",
      "Early stopping, best iteration is:\n",
      "[8509]\ttraining's auc: 0.674866\tvalid_1's auc: 0.655219\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    \n",
    "    N = 5\n",
    "    p_valid,yp = 0,0\n",
    "    for i in range(N):\n",
    "#         X_t, y_t = (X_train, y_train)\n",
    "#         X_t = pd.DataFrame(X_t)\n",
    "#         X_t = X_t.add_prefix('var_')\n",
    "        \n",
    "        trn_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val)\n",
    "      \n",
    "        evals_result = {}\n",
    "        lgb_clf = lgb.train(lgb_params,\n",
    "                        trn_data,\n",
    "                        30000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        early_stopping_rounds=3000,\n",
    "                        verbose_eval = 1000,\n",
    "                        evals_result=evals_result\n",
    "                       )\n",
    "        p_valid += lgb_clf.predict(X_val)\n",
    "        yp += lgb_clf.predict(x_test)\n",
    "    val_score = roc_auc_score(y_val, p_valid)\n",
    "    val_aucs.append(val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=yp/5\n",
    "submission = pd.DataFrame({'UniqueID': unique_id,'loan_default': predictions})\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
