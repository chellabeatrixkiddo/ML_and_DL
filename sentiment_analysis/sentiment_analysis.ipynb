{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To identify hatespeech in the tweets ##\n",
    "## Dataset obtained from Analytics Vidhya practice problem ##\n",
    "\n",
    "#train = pd.read_csv(\"~/Documents/Study/studypython/twitter_data/train_E6oV3lV.csv\")\n",
    "#test = pd.read_csv(\"~/Documents/Study/studypython/twitter_data/test_tweets_anuFYb8.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment140 twitter data from Stanford: http://help.sentiment140.com/for-students/\n",
    "\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "\n",
    "train = pd.read_csv(\"~/Documents/Study/studypython/Sentiment140/trainingandtestdata/training.1600000.processed.noemoticon.csv\", encoding = \"latin-1\", header=None, names=cols)\n",
    "test = pd.read_csv(\"~/Documents/Study/studypython/Sentiment140/trainingandtestdata/testdata.manual.2009.06.14.csv\", encoding = \"latin-1\", header=None, names=cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date query_string  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009     NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009     NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009     NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  id                          date query_string      user  \\\n",
       "0          4   3  Mon May 11 03:17:40 UTC 2009      kindle2    tpryan   \n",
       "1          4   4  Mon May 11 03:18:03 UTC 2009      kindle2    vcu451   \n",
       "2          4   5  Mon May 11 03:18:54 UTC 2009      kindle2    chadfu   \n",
       "3          4   6  Mon May 11 03:19:04 UTC 2009      kindle2     SIX15   \n",
       "4          4   7  Mon May 11 03:21:41 UTC 2009      kindle2  yamarama   \n",
       "\n",
       "                                                text  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop unwanted attributes/dimensions:\n",
    "# in this case: id, date, query_string, user attributes are not necessary\n",
    "\n",
    "train.drop(['id','date','query_string','user'],axis=1,inplace=True)\n",
    "test.drop(['id','date','query_string','user'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training set has only 0 and 4 polarity (positive and negative sentiments. there are no\n",
    "## traiing data for the neutral class) But the test set has instances belonging to all 3 classes - 0, 2 and 4\n",
    "## therefore we are removing those test instances with polarity 2 (neutral class)\n",
    "\n",
    "test = test[test['sentiment'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    182\n",
       "0    177\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the sentiment polarity values from {0,4} to {0,1} in both train and test sets:\n",
    "\n",
    "train['sentiment'] = train['sentiment'].map({0: 0, 4: 1})\n",
    "test['sentiment'] = test['sentiment'].map({0: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contraction Handling:\n",
    "\n",
    "# Mapping Contractions to their expanded forms:\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } \n",
    "\n",
    "# converting all apostrophes to single quotes:\n",
    "def apos_handling(text):\n",
    "    return re.sub(\"’\", \"'\", text)\n",
    "\n",
    "def contraction_handling(text):\n",
    "    return ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove HTML encoding:\n",
    "def remove_html_enc(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    return soup.get_text()\n",
    "\n",
    "## Remove the twitter user handles, hashtags (only the symbol #) and url links (http and www)\n",
    "pattern1 = re.compile(r\"(@[A-Za-z0-9_]*)|(#)|(www.[^ ]+)|(https?://[^ ]+)\")\n",
    "\n",
    "def clean_tweet(text):\n",
    "    return pattern1.sub(\"\", text)\n",
    "\n",
    "\n",
    "## Remove the non-ascii characters (that represent the emoticons)\n",
    "def remove_nonascii(text):\n",
    "    return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "\n",
    "## Remove UTF-BOM (Byte Order Mark) characters\n",
    "def remove_utfbom(text):\n",
    "    try:\n",
    "        clean = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = text\n",
    "    return clean\n",
    "\n",
    "\n",
    "## Remove short words whose length is <= n\n",
    "def remove_short_words(text):\n",
    "    n = 1\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([w for w in words if len(w) > n])\n",
    "\n",
    "\n",
    "## Remove non-letter characters: numbers, punctuations\n",
    "pattern2 = re.compile(\"([^A-Za-z\\s]+)\")\n",
    "\n",
    "def remove_nonletter(text):\n",
    "    return pattern2.sub(\"\", text)\n",
    "\n",
    "## Negation Handling: (subsumed in contraction handling above)\n",
    "\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "negation_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def neg_handling(text):\n",
    "    return negation_pattern.sub(lambda x : negations_dic[x.group()], text)\n",
    "\n",
    "\n",
    "## Simple Spell Correction: a character that is repeated 2 or more times is shortened to 2 repetitions\n",
    "def simple_spellcorrect(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()\n",
    "\n",
    "train['text'] = train['text'].apply(remove_html_enc)\n",
    "test['text'] = test['text'].apply(remove_html_enc)\n",
    "\n",
    "train['text'] = train['text'].apply(clean_tweet)\n",
    "test['text'] = test['text'].apply(clean_tweet)\n",
    "\n",
    "train['text'] = train['text'].apply(remove_utfbom)\n",
    "test['text'] = test['text'].apply(remove_utfbom)\n",
    "\n",
    "train['text'] = train['text'].apply(apos_handling)\n",
    "test['text'] = test['text'].apply(apos_handling)\n",
    "\n",
    "train['text'] = train['text'].apply(contraction_handling)\n",
    "test['text'] = test['text'].apply(contraction_handling)\n",
    "\n",
    "#train['text'] = train['text'].apply(neg_handling)\n",
    "#test['text'] = test['text'].apply(neg_handling)\n",
    "\n",
    "train['text'] = train['text'].apply(remove_nonletter)\n",
    "test['text'] = test['text'].apply(remove_nonletter)\n",
    "\n",
    "#train['text'] = train['text'].apply(remove_nonascii)\n",
    "#test['text'] = test['text'].apply(remove_nonascii)\n",
    "\n",
    "train['text'] = train['text'].apply(remove_short_words)\n",
    "test['text'] = test['text'].apply(remove_short_words)\n",
    "\n",
    "train['text'] = train['text'].apply(simple_spellcorrect)\n",
    "test['text'] = test['text'].apply(simple_spellcorrect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aww that is bummer you shoulda got david carr ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can not update his facebook b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dived many times for the ball managed to save ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no it is not behaving at all am mad why am her...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  aww that is bummer you shoulda got david carr ...\n",
       "1          0  is upset that he can not update his facebook b...\n",
       "2          0  dived many times for the ball managed to save ...\n",
       "3          0     my whole body feels itchy and like its on fire\n",
       "4          0  no it is not behaving at all am mad why am her..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>loovvee my kindle not that the dx is cool but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>reading my kindle love it lee childs is good read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ok first assesment of the kindle it fucking rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>you will love your kindle have had mine for fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>fair enough but have the kindle and think it i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  loovvee my kindle not that the dx is cool but ...\n",
       "1          1  reading my kindle love it lee childs is good read\n",
       "2          1  ok first assesment of the kindle it fucking rocks\n",
       "3          1  you will love your kindle have had mine for fe...\n",
       "4          1  fair enough but have the kindle and think it i..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove stop words:\n",
    "'''\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    words_wo_stop = [w for w in words if w not in stop_words]\n",
    "    return ' '.join(words_wo_stop)\n",
    "\n",
    "train['text'] = train['text'].apply(remove_stopwords)\n",
    "test['text'] = test['text'].apply(remove_stopwords)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming:\n",
    "'''\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def get_stems(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([porter_stemmer.stem(w) for w in words])\n",
    "\n",
    "train['text'] = train['text'].apply(get_stems)\n",
    "test['text'] = test['text'].apply(get_stems)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lemmatization: does it make any difference because of lack of proper POS structure in the tweets??\n",
    "\n",
    "# map the pos_tags to wordnet pos tags\n",
    "\n",
    "tagmap = defaultdict(lambda : wordnet.NOUN)\n",
    "tagmap['J'] = wordnet.ADJ\n",
    "tagmap['V'] = wordnet.VERB\n",
    "tagmap['R'] = wordnet.ADV\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_lemmas(text):\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(w, pos = tagmap[t[0]]) for w, t in (nltk.pos_tag(words))])\n",
    "\n",
    "train['text'] = train['text'].apply(get_lemmas)\n",
    "test['text'] = test['text'].apply(get_lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorization - Feature Extraction\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "train_counts = count_vect.fit_transform(train['text'])\n",
    "\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "\n",
    "test_counts = count_vect.transform(test['text'])\n",
    "\n",
    "test_tfidf = tfidf_transformer.transform(test_counts)\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', ngram_range=(1,3))\n",
    "train_tfidf = tfidf.fit_transform(train['text'])\n",
    "test_tfidf = tfidf.transform(test['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 9421254)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('naivebayes', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)), ('logistic', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solve...ty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build Classifiers: Naive Bayes, SVM, Logistic Regression, Ensemble\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "train_target = train['sentiment']\n",
    "\n",
    "# Train the classifier\n",
    "\n",
    "# 1. Naive Bayes:\n",
    "cl1 = MultinomialNB()\n",
    "cl1.fit(train_tfidf, train_target)\n",
    "\n",
    "# 2. Logistic Regression:\n",
    "cl2 = LogisticRegression()\n",
    "cl2.fit(train_tfidf, train_target)\n",
    "\n",
    "# 3. SVM:\n",
    "cl3 = SGDClassifier(loss='hinge', penalty = 'l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "cl3.fit(train_tfidf, train_target)\n",
    "\n",
    "# 4. Ensemble:\n",
    "estimators = []\n",
    "estimators.append(('naivebayes', cl1))\n",
    "estimators.append(('logistic', cl2))\n",
    "estimators.append(('svm', cl3))\n",
    "ensemble = VotingClassifier(estimators)\n",
    "ensemble.fit(train_tfidf, train_target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Predict the test data labels:\n",
    "\n",
    "pr1 = cl1.predict(test_tfidf)\n",
    "pr2 = cl2.predict(test_tfidf)\n",
    "pr3 = cl3.predict(test_tfidf)\n",
    "pr4 = ensemble.predict(test_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.7994428969359332\n",
      "Logistic Regression: 0.8161559888579387\n",
      "SVM: 0.7632311977715878\n",
      "Ensemble: 0.8189415041782729\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy of the various classifiers\n",
    "\n",
    "test_target = test['sentiment']\n",
    "\n",
    "print(\"Naive Bayes:\", accuracy_score(test_target, pr1))\n",
    "print(\"Logistic Regression:\", accuracy_score(test_target, pr2))\n",
    "print(\"SVM:\", accuracy_score(test_target, pr3))\n",
    "print(\"Ensemble:\", accuracy_score(test_target, pr4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    182\n",
       "0    177\n",
       "2    139\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test['sentiment'].unique()\n",
    "\n",
    "test['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idcol = test[\"id\"]\n",
    "result1 = pd.DataFrame({\"id\": idcol, \"label\": pr1})\n",
    "result1.to_csv(\"~/Documents/Study/studypython/twitter_data/nb.csv\", sep = \",\", index=False, columns=[\"id\", \"sentiment\"])\n",
    "\n",
    "result2 = pd.DataFrame({\"id\": idcol, \"label\": pr2})\n",
    "result2.to_csv(\"~/Documents/Study/studypython/twitter_data/lr.csv\", sep = \",\", index=False, columns=[\"id\", \"sentiment\"])\n",
    "\n",
    "result3 = pd.DataFrame({\"id\": idcol, \"label\": pr3})\n",
    "result3.to_csv(\"~/Documents/Study/studypython/twitter_data/svm.csv\", sep = \",\", index=False, columns=[\"id\", \"sentiment\"])\n",
    "\n",
    "result4 = pd.DataFrame({\"id\": idcol, \"label\": pr4})\n",
    "result4.to_csv(\"~/Documents/Study/studypython/twitter_data/ensemble.csv\", sep = \",\", index=False, columns=[\"id\", \"sentiment\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram All the 363979 features: \n",
    "Naive Bayes: 0.7715877437325905\n",
    "Logistic Regression: 0.7938718662952646\n",
    "SVM: 0.7437325905292479\n",
    "Ensemble: 0.7994428969359332\n",
    "\n",
    "# Unigram 50000 features:\n",
    "Naive Bayes: 0.7827298050139275\n",
    "Logistic Regression: 0.7883008356545961\n",
    "SVM: 0.7520891364902507\n",
    "Ensemble: 0.7994428969359332\n",
    "\n",
    "# Unigram 10000 features:\n",
    "Naive Bayes: 0.7994428969359332\n",
    "Logistic Regression: 0.7994428969359332\n",
    "SVM: 0.766016713091922\n",
    "Ensemble: 0.8161559888579387\n",
    "\n",
    "# Unigram 5000 features:\n",
    "Naive Bayes: 0.8133704735376045\n",
    "Logistic Regression: 0.8133704735376045\n",
    "SVM: 0.7715877437325905\n",
    "Ensemble: 0.8217270194986073\n",
    "\n",
    "# Unigram 2500 features:\n",
    "Naive Bayes: 0.7910863509749304\n",
    "Logistic Regression: 0.7938718662952646\n",
    "SVM: 0.766016713091922\n",
    "Ensemble: 0.7938718662952646\n",
    "\n",
    "# Unigram 1000 features:\n",
    "Naive Bayes: 0.7827298050139275\n",
    "Logistic Regression: 0.7771587743732591\n",
    "SVM: 0.7520891364902507\n",
    "Ensemble: 0.7771587743732591\n",
    "\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "# Bigram All 3794859 features:\n",
    "Naive Bayes: 0.7910863509749304\n",
    "Logistic Regression: 0.8245125348189415\n",
    "SVM: 0.7715877437325905\n",
    "Ensemble: 0.8189415041782729\n",
    "\n",
    "# Bigram 20000 features:\n",
    "Naive Bayes: 0.7855153203342619\n",
    "Logistic Regression: 0.8105849582172702\n",
    "SVM: 0.7632311977715878\n",
    "Ensemble: 0.8022284122562674\n",
    "\n",
    "# Bigram 10000 features:\n",
    "Naive Bayes: 0.7910863509749304\n",
    "Logistic Regression: 0.8161559888579387\n",
    "SVM: 0.7632311977715878\n",
    "Ensemble: 0.8133704735376045\n",
    "\n",
    "# Bigram 5000 features:\n",
    "Naive Bayes: 0.7883008356545961\n",
    "Logistic Regression: 0.807799442896936\n",
    "SVM: 0.766016713091922\n",
    "Ensemble: 0.7966573816155988\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "# Trigram All 9421254 features:\n",
    "Naive Bayes: 0.7994428969359332\n",
    "Logistic Regression: 0.8161559888579387\n",
    "SVM: 0.7632311977715878\n",
    "Ensemble: 0.8189415041782729\n",
    "\n",
    "# Trigram 20000 features:\n",
    "Naive Bayes: 0.7855153203342619\n",
    "Logistic Regression: 0.8105849582172702\n",
    "SVM: 0.766016713091922\n",
    "Ensemble: 0.7994428969359332\n",
    "\n",
    "# Trigram 10000 features:\n",
    "Naive Bayes: 0.7883008356545961\n",
    "Logistic Regression: 0.8161559888579387\n",
    "SVM: 0.7632311977715878\n",
    "Ensemble: 0.8105849582172702\n",
    "\n",
    "# Trigram 5000 features:\n",
    "Naive Bayes: 0.7855153203342619\n",
    "Logistic Regression: 0.807799442896936\n",
    "SVM: 0.766016713091922\n",
    "Ensemble: 0.7938718662952646\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
