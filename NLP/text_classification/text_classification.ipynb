{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname = \"/Users/Admin/Documents/Study/studypython/movie_data\"\n",
    "\n",
    "file1 = open(pathname+\"/full_train.txt\", \"r\", encoding=\"utf-8\")\n",
    "file2 = open(pathname+\"/full_test.txt\", \"r\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = []\n",
    "\n",
    "for line in file1:\n",
    "    train_reviews.append(line.strip())\n",
    "\n",
    "test_reviews = []\n",
    "\n",
    "for line in file2:\n",
    "    test_reviews.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Preprocessing ##\n",
    "import re\n",
    "\n",
    "# 1. Remove punctuations and special characters\n",
    "pattern1 = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "pattern2 = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "train_reviews = [pattern1.sub(\"\", item.lower()) for item in train_reviews]\n",
    "train_reviews = [pattern2.sub(\" \", item.lower()) for item in train_reviews]\n",
    "\n",
    "test_reviews = [pattern1.sub(\"\", item.lower()) for item in test_reviews]\n",
    "test_reviews = [pattern2.sub(\" \", item.lower()) for item in test_reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 2. Remove Stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train_reviews_wo_stop = []\n",
    "\n",
    "for item in train_reviews:\n",
    "    words = word_tokenize(item)\n",
    "    words_wo_stop = [w for w in words if w not in stop_words]\n",
    "    item_wo_stop = ' '.join(words_wo_stop)\n",
    "    train_reviews_wo_stop.append(item_wo_stop)\n",
    "\n",
    "test_reviews_wo_stop = []\n",
    "\n",
    "for item in test_reviews:\n",
    "    words = word_tokenize(item)\n",
    "    words_wo_stop = [w for w in words if w not in stop_words]\n",
    "    item_wo_stop = ' '.join(words_wo_stop)\n",
    "    test_reviews_wo_stop.append(item_wo_stop)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews_wo_stop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. a. Stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "train_reviews_wostop_stemmed = []\n",
    "\n",
    "for item in train_reviews_wo_stop:\n",
    "    words = word_tokenize(item)\n",
    "    stem_words = [porter_stemmer.stem(w) for w in words]\n",
    "    item_stem = ' '.join(stem_words)\n",
    "    train_reviews_wostop_stemmed.append(item_stem)\n",
    "\n",
    "test_reviews_wostop_stemmed = []\n",
    "\n",
    "for item in test_reviews_wo_stop:\n",
    "    words = word_tokenize(item)\n",
    "    stem_words = [porter_stemmer.stem(w) for w in words]\n",
    "    item_stem = ' '.join(stem_words)\n",
    "    test_reviews_wostop_stemmed.append(item_stem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went saw movi last night coax friend mine ill admit reluct see knew ashton kutcher abl comedi wrong kutcher play charact jake fischer well kevin costner play ben randal profession sign good movi toy emot one exactli entir theater sold overcom laughter first half movi move tear second half exit theater saw mani women tear mani full grown men well tri desper let anyon see cri movi great suggest go see judg\n"
     ]
    }
   ],
   "source": [
    "print(test_reviews_wostop_stemmed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Engineering - Vectorization ##\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "train_reviews_counts = count_vect.fit_transform(train_reviews_wostop_stemmed)\n",
    "\n",
    "train_reviews_tfidf = tfidf_transformer.fit_transform(train_reviews_counts)\n",
    "\n",
    "test_reviews_counts = count_vect.transform(test_reviews_wostop_stemmed)\n",
    "\n",
    "test_reviews_tfidf = tfidf_transformer.transform(test_reviews_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 65269)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reviews_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Building a Classifier ##\n",
    "\n",
    "# target labels for the training and test data : the first 12.5k are positive and the last 12.5k are negative\n",
    "train_target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "test_target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "# Train the classifier\n",
    "# 1. Naive Bayes:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cl1 = MultinomialNB()\n",
    "cl1.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 2. Logistic Regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cl2 = LogisticRegression(C = 0.05)\n",
    "cl2.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 3. SVM:\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "cl3 = SGDClassifier(loss='hinge', penalty = 'l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "cl3.fit(train_reviews_tfidf, train_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test data labels:\n",
    "\n",
    "pr1 = cl1.predict(test_reviews_tfidf)\n",
    "pr2 = cl2.predict(test_reviews_tfidf)\n",
    "pr3 = cl3.predict(test_reviews_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.82064\n",
      "Logistic Regression: 0.84772\n",
      "SVM: 0.85288\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy of the various classifiers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Naive Bayes:\", accuracy_score(test_target, pr1))\n",
    "print(\"Logistic Regression:\", accuracy_score(test_target, pr2))\n",
    "print(\"SVM:\", accuracy_score(test_target, pr3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to convert pos_tag to wordnet compatible pos tags ##\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "tag_map['J'] = wordnet.ADJ\n",
    "tag_map['V'] = wordnet.VERB\n",
    "tag_map['R'] = wordnet.ADV\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### With Lemmatization #######\n",
    "\n",
    "# 3. b. Lemmatization\n",
    "\n",
    "# POS Tagging: (In order to lemmatize we need the POS tags)\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "train_reviews_wostop_lemma = []\n",
    "\n",
    "for item in train_reviews_wo_stop:\n",
    "    words = word_tokenize(item)\n",
    "    postags = nltk.pos_tag(words)\n",
    "    wordlist, taglist = zip(*postags)\n",
    "    lemmas = [lemmatizer.lemmatize(w, pos = tag_map[t[0]]) for w,t in zip(wordlist,taglist)]\n",
    "    item_lemmas = ' '.join(lemmas)\n",
    "    train_reviews_wostop_lemma.append(item_lemmas)\n",
    "    \n",
    "test_reviews_wostop_lemma = []\n",
    "\n",
    "for item in test_reviews_wo_stop:\n",
    "    words = word_tokenize(item)\n",
    "    postags = nltk.pos_tag(words)\n",
    "    wordlist, taglist = zip(*postags)\n",
    "    lemmas = [lemmatizer.lemmatize(w, pos = tag_map[t[0]]) for w,t in zip(wordlist,taglist)]\n",
    "    item_lemmas = ' '.join(lemmas)\n",
    "    test_reviews_wostop_lemma.append(item_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go saw movie last night coax friend mine ill admit reluctant see knew ashton kutcher able comedy wrong kutcher played character jake fischer well kevin costner play ben randall professionalism sign good movie toy emotion one exactly entire theater sell overcome laughter first half movie move tear second half exit theater saw many woman tear many full grow men well try desperately let anyone see crying movie great suggest go see judge\n"
     ]
    }
   ],
   "source": [
    "print(test_reviews_wostop_lemma[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Engineering - Vectorization ##\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "train_reviews_counts = count_vect.fit_transform(train_reviews_wostop_lemma)\n",
    "\n",
    "train_reviews_tfidf = tfidf_transformer.fit_transform(train_reviews_counts)\n",
    "\n",
    "test_reviews_counts = count_vect.transform(test_reviews_wostop_lemma)\n",
    "\n",
    "test_reviews_tfidf = tfidf_transformer.transform(test_reviews_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 82993)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Building a Classifier ##\n",
    "\n",
    "# Train the classifier\n",
    "# 1. Naive Bayes:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cl1 = MultinomialNB()\n",
    "cl1.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 2. Logistic Regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cl2 = LogisticRegression(C = 0.05)\n",
    "cl2.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 3. SVM:\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "cl3 = SGDClassifier(loss='hinge', penalty = 'l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "cl3.fit(train_reviews_tfidf, train_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test data labels:\n",
    "\n",
    "pr1 = cl1.predict(test_reviews_tfidf)\n",
    "pr2 = cl2.predict(test_reviews_tfidf)\n",
    "pr3 = cl3.predict(test_reviews_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.82536\n",
      "Logistic Regression: 0.84236\n",
      "SVM: 0.84464\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy of the various classifiers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Naive Bayes:\", accuracy_score(test_target, pr1))\n",
    "print(\"Logistic Regression:\", accuracy_score(test_target, pr2))\n",
    "print(\"SVM:\", accuracy_score(test_target, pr3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### So as we can see, Lemmatizing doesn't seem to have improved the accuracy that much.In fact in the case of Logistic\n",
    "# Regression and SVM, the accuracy has decreased. ####\n",
    "\n",
    "## Let us Vectorize with n-gram (previously it was unigram) - getting a Document-Term TF_IDF matrix with\n",
    "# bigram - using the original text (only applying stop word removal and NOT DOING stemming/lemmatization)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', ngram_range=(1,2))\n",
    "train_reviews_tfidf = tfidf.fit_transform(train_reviews)\n",
    "test_reviews_tfidf = tfidf.transform(test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Building a Classifier ##\n",
    "\n",
    "# Train the classifier\n",
    "# 1. Naive Bayes:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cl1 = MultinomialNB()\n",
    "cl1.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 2. Logistic Regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cl2 = LogisticRegression(C = 0.05)\n",
    "cl2.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 3. SVM:\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "cl3 = SGDClassifier(loss='hinge', penalty = 'l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "cl3.fit(train_reviews_tfidf, train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test data labels:\n",
    "\n",
    "pr1 = cl1.predict(test_reviews_tfidf)\n",
    "pr2 = cl2.predict(test_reviews_tfidf)\n",
    "pr3 = cl3.predict(test_reviews_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.8548\n",
      "Logistic Regression: 0.83268\n",
      "SVM: 0.79808\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy of the various classifiers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Naive Bayes:\", accuracy_score(test_target, pr1))\n",
    "print(\"Logistic Regression:\", accuracy_score(test_target, pr2))\n",
    "print(\"SVM:\", accuracy_score(test_target, pr3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.85592\n",
      "Logistic Regression: 0.8338\n",
      "SVM: 0.80456\n"
     ]
    }
   ],
   "source": [
    "## Let us Vectorize with n-gram (previously it was unigram) - getting a Document-Term TF_IDF matrix with\n",
    "# bigram - using the original text (applying stop word removal and lemmatization)\n",
    "\n",
    "train_reviews_wostop_lemma\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "train_reviews_tfidf = tfidf.fit_transform(train_reviews_wostop_lemma)\n",
    "test_reviews_tfidf = tfidf.transform(test_reviews_wostop_lemma)\n",
    "\n",
    "### Building a Classifier ##\n",
    "\n",
    "# Train the classifier\n",
    "# 1. Naive Bayes:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cl1 = MultinomialNB()\n",
    "cl1.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 2. Logistic Regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cl2 = LogisticRegression(C = 0.05)\n",
    "cl2.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 3. SVM:\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "cl3 = SGDClassifier(loss='hinge', penalty = 'l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "cl3.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# Predict the test data labels:\n",
    "\n",
    "pr1 = cl1.predict(test_reviews_tfidf)\n",
    "pr2 = cl2.predict(test_reviews_tfidf)\n",
    "pr3 = cl3.predict(test_reviews_tfidf)\n",
    "\n",
    "# Find the accuracy of the various classifiers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Naive Bayes:\", accuracy_score(test_target, pr1))\n",
    "print(\"Logistic Regression:\", accuracy_score(test_target, pr2))\n",
    "print(\"SVM:\", accuracy_score(test_target, pr3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: 0.85928\n",
      "Logistic Regression: 0.82944\n",
      "SVM: 0.82068\n"
     ]
    }
   ],
   "source": [
    "## Let us Vectorize with n-gram (previously it was unigram) - getting a Document-Term TF_IDF matrix with\n",
    "# trigram - using the original text (applying stop word removal and lemmatization)\n",
    "\n",
    "train_reviews_wostop_lemma\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "train_reviews_tfidf = tfidf.fit_transform(train_reviews_wostop_lemma)\n",
    "test_reviews_tfidf = tfidf.transform(test_reviews_wostop_lemma)\n",
    "\n",
    "### Building a Classifier ##\n",
    "\n",
    "# Train the classifier\n",
    "# 1. Naive Bayes:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cl1 = MultinomialNB()\n",
    "cl1.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 2. Logistic Regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "cl2 = LogisticRegression(C = 0.05)\n",
    "cl2.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# 3. SVM:\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "cl3 = SGDClassifier(loss='hinge', penalty = 'l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "cl3.fit(train_reviews_tfidf, train_target)\n",
    "\n",
    "# Predict the test data labels:\n",
    "\n",
    "pr1 = cl1.predict(test_reviews_tfidf)\n",
    "pr2 = cl2.predict(test_reviews_tfidf)\n",
    "pr3 = cl3.predict(test_reviews_tfidf)\n",
    "\n",
    "# Find the accuracy of the various classifiers\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Naive Bayes:\", accuracy_score(test_target, pr1))\n",
    "print(\"Logistic Regression:\", accuracy_score(test_target, pr2))\n",
    "print(\"SVM:\", accuracy_score(test_target, pr3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the Results:\n",
    "\n",
    "\n",
    "## Remove Stop Words + Stemming (Porter) + Unigram TF-IDF\n",
    "\n",
    "### Naive Bayes: 0.82064     Logistic Regression: 0.84772    SVM: 0.85288\n",
    "\n",
    "\n",
    "\n",
    "## Remove Stop Words + Lemmatization (Wordnet) + Unigram TF-IDF\n",
    "\n",
    "### Naive Bayes: 0.82536    Logistic Regression: 0.84236    SVM: 0.84464\n",
    "\n",
    "\n",
    "\n",
    "## Remove Stop Words + Bigram TF-IDF\n",
    "\n",
    "### Naive Bayes: 0.8548    Logistic Regression: 0.83268    SVM: 0.79808\n",
    "\n",
    "\n",
    "\n",
    "## Remove Stop Words + Lemmatization (Wordnet) + Bigram TF-IDF\n",
    "\n",
    "### Naive Bayes: 0.85592    Logistic Regression: 0.8338    SVM: 0.80456\n",
    "\n",
    "\n",
    "\n",
    "## Remove Stop Words + Lemmatization (Wordnet) + Trigram TF-IDF\n",
    "\n",
    "### Naive Bayes: 0.85928    Logistic Regression: 0.82944    SVM: 0.82068"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
